{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AISLE', 'CORNER', 'INTERSECTION', 'OPEN AREA']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "from PIL import Image as PILImage\n",
    "from Prompts import *\n",
    "import pickle as pkl\n",
    "import yaml\n",
    "from utils.config import *\n",
    "from utils.models import *\n",
    "import copy\n",
    "import subprocess\n",
    "model = GPTModel(config = dict(\n",
    "    MODEL_NAME = MODEL\n",
    "))\n",
    "with open(os.path.join('locations',LOCATION,'scene_graph.json'),'r') as f:\n",
    "    scene_graph = json.load(f)\n",
    "\n",
    "scgraph = utils.SceneGraph(scene_graph)\n",
    "node_types = []\n",
    "for node in scgraph.get_parent_nodes():\n",
    "    if scgraph.graph.nodes[node]['type'] not in node_types:\n",
    "        node_types.append(scgraph.graph.nodes[node]['type'])\n",
    "print(node_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scQ = ScenarioQuery()\n",
    "scQ_full_prompt = scQ.get_full_prompt(context=CONTEXT,\n",
    "    task=TASK,\n",
    "    rough_scenario=ROUGH_SCENARIO,\n",
    "    location=LOCATION_DESC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': ' Social Navigation is a complex skill for a robot to accomplish and the appropriateness of the behavior of a robot is highly dependent on the task and the social context. Thus a robot’s social navigation capabilities must be thoroughly tested, and this is done by evaluating the robot’s behavior in a number of scenarios in a variety of contexts.\\n    \\nYou are a scenario designer. There is only 1 human in the scene. Your task is to generate scenarios to test the social navigation capabilities of a robot.\\nA Social Navigation [Scenario] is defined by:\\n    1. Scenario Description: very detailed description of the scenario. WHAT happens in the scenario and WHERE the scenario takes place. WHERE the robot and humans are located.\\n    2. Human Behavior:  how human interacts with the robot when it is visible, for e.g. Human 1 is scared of the robot and asks it to stop, Human 2 doesn\\'t notice the robot at all etc.\\nYour output description will be later used by an expert Behaviour tree designer to generate a Behavior Tree for each human in the scene. The behavior tree designer is not allowed to modify the scenario and can only create behavior that can obtained from the following Actions and Conditions:\\n - Conditions\\n        - Check the visibility of the robot\\n        - Check if the human has reached their goal\\n        - Check if robot is saying any particular phrase\\n    \\n    - Actions:\\n        - Make the human perform a gesture.\\n        - Make the human perform normal navigation to reach its goal and treat the robot as a normal obstalce. This is regular behavior for humans.\\n        - Make the human just stand and watch the robot if it is nearby\\n        - Make the human curious about the robot and stand and watch the robot if its nearby, otherwise makes the human go slowly towards the robot until it reaches whithin a certain distance.\\n        - Make the human scared of the robot and overly avoid it.\\n        - Make the human threaten the robot move quickly towards the front of the robot to block it.\\n        NOTE: AT ANY GIVEN POINT OF TIME, THE HUMAN CAN ONlY PEFORM ANY ONE OF THE ABOVE ACTIONS.\\n\\nThe humans are only capable of performing the actions mentioned above.\\nUser will provide a [Social context], a [Task] that the robot needs to do, a description of the location and optionally a [Rough Scenario]. \\nYour generated scenario will be programmatically simulated through a pipeline into a scenario in the Gazebo physics simulator.\\nRules:\\n- Describe human behavior by describing their [Human Task] and their [Behavior Towards Robot].\\n- The humans can say \"WAIT\", \"PROCEED\", \"EXCUSE ME\" to the robot  to aid in navigation. The robot can say \"WAIT\", \"PROCEED\", \"EXCUSE ME\", \"ACKNOWLEDGED\" to the humans to aid in navigation. \\n- When the user provides a Rough Scenario, ensure your final scenario is strictly aligned to the rough scenario\\n- The humans in the simulator are SIMPLIFIED OBJECTS that only can move in 2D, send and receive simple phrases, detect and simulate looking at the robot, group together with other humans, navigate to a predefined goal and change their trajectory conditioned on the robot’s position and velocity.\\n- When using groups in the scenario, add all group members to the humans in the scenario. Having only 1 human with \\'INTERACTING WITH GROUP\\' task is incorrect.\\n\\n\\n\\n\\nYOU ADHERE TO THE FOLLOWING JSON FORMAT STRICTLY. \\n{\\n\\'Scenario Description\\': <very detailed description of the scenario >,\\n\\'Number of Humans\\': <Number of humans that are involved in the scenario>,\\n‘Human behavior\\': {\\n‘Human 1’: <Describe the behavior of Human 1>,\\n‘Human 2’: <Describe the behavior of Human 2>,\\n},\\n\\'Expected Robot Behavior\\': <Describe the behavior expected from the robot>\\n}\\n    \\n### Example:\\nUser:\\n[Social context]: Robot is a home assistant in a Singaporean old-age home and performs daily helpful duties for the residents\\n[Robot Task]: Deliver coffee\\n[Rough Scenario]: None\\n[Location]:   The home has a Kitchen, a Bedroom and a Living Room. A doorway connects the Kitchen to the Living Room and a Passageway connects the Living Room and the Doorway.\\nAssistant:\\n{\\'Scenario Description\\': \"The robot is trying to deliver coffee from the Kitchen to the Living Room and encounters one of the elderly residents entering the Kitchen from the Living Room through the  Doorway.\",\\n\\'Number of Humans\\': 1,\\n\\'Human Behavior\\':{\\n\\'Human 1\\': Human 1 is going from going to kitchen from the living room. If the robot is very close-by, Human asks the robot to stop and waits (for a maximum of 5s) for the robot to stop, then continues navigating. Ignores the robot if it asks the human to wait.\\n},\\n\\'Expected Robot Behavior\\': \"The robot says \"I AM HERE\" to the resident. It waits for the resident to be well clear of the Doorway before going through the Doorway to the Living Room in a slow pace.\"\\n}\\n###\\n    \\n\\n                    USER:\\n                    [Social Context]:Robot is a delivery bot in a small warehouse. There are employees performing daily duties walking around the warehouse.\\n                    [Task]:Robot has to transport boxes in the warehouse\\n                    [Location]:The Location is a Small Warehouse consists of various racks and open packaging areas connected by Passageways. The Passageways also create Intersections.\\n                    [Rough Scenario]: The robot starts from a narrow passageway and needs to transport a box in the warehouse. While coming out of the narrow passageway the robot encounters a warehouse employee moving from the opposite end of the narrow passageway towards the robot. \\n                    Assistant:\\n                    '}]}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scQ_full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Scenario Description', 'Number of Humans', 'Human Behavior', 'Expected Robot Behavior'])\n",
      "{'Expected Robot Behavior': 'The robot, upon encountering the employee in the '\n",
      "                            'narrow passageway, should stop and say '\n",
      "                            \"'ACKNOWLEDGED' to allow the employee to pass \"\n",
      "                            'first. Once the employee has moved past the '\n",
      "                            'robot, it should then proceed to transport the '\n",
      "                            'box to its destination in the warehouse in a '\n",
      "                            'controlled and cautious manner.',\n",
      " 'Human Behavior': {'Human 1': {'Behavior Towards Robot': 'If the robot is '\n",
      "                                                          'visible and '\n",
      "                                                          'close-by, the '\n",
      "                                                          'employee stops and '\n",
      "                                                          \"says 'WAIT'. If the \"\n",
      "                                                          'robot stops and '\n",
      "                                                          'acknowledges by '\n",
      "                                                          'saying '\n",
      "                                                          \"'ACKNOWLEDGED', the \"\n",
      "                                                          'employee proceeds '\n",
      "                                                          'slowly. If the '\n",
      "                                                          'robot does not '\n",
      "                                                          'respond or the '\n",
      "                                                          'employee does not '\n",
      "                                                          'see the robot, the '\n",
      "                                                          'employee treats the '\n",
      "                                                          'robot as a normal '\n",
      "                                                          'obstacle and '\n",
      "                                                          'navigates around it '\n",
      "                                                          'if possible.',\n",
      "                                'Human Task': 'The warehouse employee needs to '\n",
      "                                              'reach the packaging area on the '\n",
      "                                              'other side of the passageway.'}},\n",
      " 'Number of Humans': 1,\n",
      " 'Scenario Description': 'The robot starts in a narrow passageway carrying a '\n",
      "                         'box and needs to navigate to the other end of the '\n",
      "                         'warehouse. When coming out of the narrow passageway, '\n",
      "                         'it encounters a warehouse employee moving from the '\n",
      "                         'opposite end of the narrow passageway towards the '\n",
      "                         'robot. The passageway is too narrow for both to pass '\n",
      "                         'simultaneously, requiring one to yield.'}\n"
     ]
    }
   ],
   "source": [
    "if QUERY_SC:\n",
    "    response = model.get_response(messages = scQ_full_prompt,format = \"json_object\")\n",
    "    scq_response_json = json.loads(response)\n",
    "    if SAVE_SC_RESPONSE:\n",
    "        with open('responses/reponse_sc.json','w') as f:\n",
    "            json.dump(scq_response_json,f)\n",
    "else:\n",
    "    assert LOAD_SC_RESPONSE == True\n",
    "    with open('responses/reponse_sc.json','r') as f:\n",
    "        scq_response_json = json.load(f)\n",
    "print(scq_response_json.keys())\n",
    "pprint.pprint(scq_response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HANDCRAFTED_SCENARIO:\n",
    "    scenario_desc = scenario_desc_hc\n",
    "    num_humans = num_humans_hc\n",
    "    traj_desc = traj_desc_hc\n",
    "    behav_desc = behav_desc_hc\n",
    "else:\n",
    "    scenario_desc = scq_response_json['Scenario Description']\n",
    "    num_humans = scq_response_json['Number of Humans']\n",
    "    #traj_desc = scq_response_json['Trajectories']\n",
    "    #behav_desc = scq_response_json['Behaviors']\n",
    "    human_task = scq_response_json['Human Task']\n",
    "    reaction_to_robot = scq_response_json['Reaction to Robot']\n",
    "\n",
    "# Assume the output is formatted correctly\n",
    "expected_robot_behav_desc = scq_response_json['Expected Robot Behavior'] \n",
    "#pranking_desc = scq_response_json['Principle Ranking'] \n",
    "#reasoning_desc = scq_response_json['Reasoning'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Locations Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_desc = \"\"\"The robot starts from a narrow passageway (from Node 99) with a box to transport and needs to navigate through the warehouse. As it exits the passageway, it encounters an employee coming from the opposite end of the narrow passageway, moving directly towards the robot. The passageway leads into an open area with multiple intersections branching off into other sections of the warehouse. The robot needs to decide whether to stop or proceed based on the human's behavior.\"\"\"\n",
    "num_humans = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'scene_graph_img'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m flocationQ\u001b[38;5;241m=\u001b[39m \u001b[43mFLocationQuery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscene_graph_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m flq_full_text_prompt \u001b[38;5;241m=\u001b[39m flocationQ\u001b[38;5;241m.\u001b[39mget_full_prompt(\n\u001b[1;32m      5\u001b[0m     sc_desc \u001b[38;5;241m=\u001b[39m scenario_desc,\n\u001b[1;32m      6\u001b[0m     scene_graph_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(node_types)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,child\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     num_humans \u001b[38;5;241m=\u001b[39m num_humans,\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'scene_graph_img'"
     ]
    }
   ],
   "source": [
    "flocationQ= FLocationQuery(\n",
    "    scene_graph_img = None\n",
    ")\n",
    "flq_full_text_prompt = flocationQ.get_full_prompt(\n",
    "    sc_desc = scenario_desc,\n",
    "    scene_graph_nodes = ','.join(node_types)+',child',\n",
    "    num_humans = num_humans,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use get files\n",
    "sg_json_file = model.get_files_by_name('scene_graph.json')\n",
    "sg_img_file = model.get_files_by_name('scene_graph.png')\n",
    "if sg_json_file!=None:\n",
    "    print(\"Loaded Json SG file\")\n",
    "else:\n",
    "    print(\"Creating JSON SG file\")\n",
    "    sg_json_file = model.create_file(os.path.join('locations',LOCATION,'scene_graph.json'), purpose = 'assistants')\n",
    "\n",
    "if sg_img_file!=None:\n",
    "    print(\"Loaded Image SG file\")\n",
    "else:\n",
    "    print(\"Creating Image SG file\")\n",
    "    sg_img_file = model.create_file(os.path.join('locations',LOCATION,'scene_graph.png'), purpose = 'vision')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_content = [\n",
    "            {\n",
    "             \"type\":\"text\",\n",
    "             \"text\":flq_full_text_prompt\n",
    "            },\n",
    "            {\n",
    "                \"type\":\"image_file\",\n",
    "                \"image_file\":{\n",
    "                    \"file_id\":sg_img_file.id,\n",
    "                    \"detail\":\"high\"\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "location_attachments = [\n",
    "             {\"file_id\":sg_json_file.id,\"tools\":[{\"type\":\"file_search\"}]}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get assistant\n",
    "location_assistant = model.get_assistant_by_name('location_scenario_assistant')\n",
    "if location_assistant == None:\n",
    "    print(\"Creating New Assistant\")\n",
    "    #create assistant\n",
    "    location_assistant = model.client.beta.assistants.create(\n",
    "        instructions=\"You are an expert floor planner and a software engineer. When asked a question, always return an answer that is fully parseable with python json.loads\",\n",
    "        name=\"location_scenario_assistant\",\n",
    "        tools=[{\"type\": \"file_search\"}],\n",
    "        model=model.model_name,\n",
    "    )\n",
    "else:\n",
    "    print(\"Loaded Assistant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id, run_id = model.create_and_run_thread(\n",
    "    #content = location_content,\n",
    "    content = [\n",
    "      {\n",
    "          \"type\":\"text\",\n",
    "          \"text\":\"\"\"\n",
    "          The image shows a location which is represented by a scene graph. A scene graph which is a graph with nodes (numbered red circles) representing locations and edges (blue lines) connecting them. \n",
    "A person/robot can only move from one node to another if the two nodes are connected by an edge. \n",
    "The scene graph in json file format is also attached, where each node has an unique id, a type, the pixel position of the node in the image (pos). \n",
    "The type of a node is one among [<NODE TYPES>]. The non-child nodes represent larger areas while the child nodes represent specific places within a larger area. \n",
    "The graph is bidirectional and each edge is also represented with the edge list 'links' in the json file with double sided arrows '<->'.  \n",
    "Remember that a node can only be reached from another node if they have an edge between them in the scene graph json file.\n",
    "Which nodes in the graph are of type CORNER and which nodes are each of those connected directly to?\"\"\"\n",
    "      }  \n",
    "    ],\n",
    "    attachments = location_attachments,\n",
    "    assistant_id = location_assistant.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response = model.poll_run_result(thread_id,run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Fine Locations from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_LOC:\n",
    "    payload = model.get_payload(content = flq_full_prompt)\n",
    "    response = model.get_response(messages = payload,format = 'json_object')\n",
    "    flq_response_json = json.loads(response)\n",
    "    if SAVE_LOC_RESPONSE:\n",
    "        with open('responses/reponse_loc.json','w') as f:\n",
    "            json.dump(flq_response_json,f)\n",
    "else:\n",
    "    assert LOAD_LOC_RESPONSE == True\n",
    "    with open('responses/reponse_loc.json','r') as f:\n",
    "        flq_response_json = json.load(f)\n",
    "print(flq_response_json.keys())\n",
    "pprint.pprint(flq_response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = {}\n",
    "for k,v in flq_response_json['Trajectory'].items():\n",
    "    trajectories[k] = v.split(',')\n",
    "print(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard coded for current small warehouse map scene\n",
    "def pix2world(px):\n",
    "    return [(px[0]/3.0) * 0.050000 + -7.000 ,-1*((px[1]/3.0) * 0.050000 + -10.500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_positions = {}\n",
    "for node in scene_graph['nodes']:\n",
    "    nodes_positions[node['id']] = pix2world(node['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories_world_coords = {}\n",
    "for k,v in trajectories.items():\n",
    "    trajectories_world_coords[k] = []\n",
    "    for loc in v:\n",
    "        trajectories_world_coords[k].append(nodes_positions[loc])\n",
    "print(trajectories_world_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add fine locations to sim yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_yaml = {'hunav_loader': {'ros__parameters': {'map': LOCATION,\n",
    "   'publish_people': True,\n",
    "   'agents': []}}}\n",
    "blank_human = {'id': None,\n",
    "    'skin': 0,\n",
    "    'behavior': 0,\n",
    "    'group_id': -1,\n",
    "    'max_vel': 1.5,\n",
    "    'radius': 0.4,\n",
    "    'init_pose': {'x': None, 'y': None, 'z': 1.25, 'h': 0.0},\n",
    "    'goal_radius': 0.3,\n",
    "    'cyclic_goals': False,\n",
    "    'goals': [],\n",
    "    }\n",
    "agents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_humans):\n",
    "    agents_yaml['hunav_loader']['ros__parameters']['agents'].append(f'agent{i}')\n",
    "    agents[f'agent{i}'] = copy.deepcopy(blank_human)\n",
    "    agents[f'agent{i}']['id'] = i\n",
    "    agents[f'agent{i}']['behavior'] = 7+i\n",
    "    for j,g in enumerate(trajectories_world_coords[f'Human {i+1}']):\n",
    "        if j == 0:\n",
    "            agents[f'agent{i}']['init_pose'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'z':1.25,\n",
    "                'h':0.0,\n",
    "            } \n",
    "        else:\n",
    "            agents[f'agent{i}']['goals'].append(f'g{j}')\n",
    "            agents[f'agent{i}'][f'g{j}'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'h':1.25\n",
    "            }\n",
    "agents_yaml['hunav_loader']['ros__parameters'].update(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot_start_world_coords = pix2world(nodes_positions[int(traj_desc['Robot'][0])])\n",
    "with open(HUNAV_SIM_AGENTS_FILE,'w') as f:\n",
    "    yaml.dump(agents_yaml,f)\n",
    "with open(os.path.join(HUNAV_GAZEBO_WRAPPER_DIR,'config','robot.yaml'),'w') as f:\n",
    "    yaml.dump({\n",
    "    'x_pose': robot_start_world_coords[0],\n",
    "    'y_pose': robot_start_world_coords[1]\n",
    "},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM for BT for humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_desc = {\n",
    "    'Human 1':\"\"\"\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "behav_desc['Human 1'] = 'Follows the robot from location 1 to location 2 for inventory check.'\n",
    "behav_desc['Human 2'] = 'Crosses the path of the robot and Human 1 at location 3.'\n",
    "behav_desc['Human 3'] = 'Moves in the same direction as the robot between location 4 and location 5 but faster, potentially overtaking.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_node_requests = []\n",
    "for i in range(num_humans):\n",
    "    btq = BTQuery()\n",
    "    if QUERY_BT:\n",
    "        print(behav_desc[f'Human {i+1}'])\n",
    "        btq_full_prompt = btq.get_full_prompt(behavior = behav_desc[f'Human {i+1}'])\n",
    "        payload = model.get_payload(content = btq_full_prompt)\n",
    "        response = model.get_response(messages = payload,format = 'json_object')\n",
    "        btq_response_json = json.loads(response)\n",
    "        if SAVE_BT_RESPONSE:\n",
    "            with open(f'responses/reponse_bt_{i+1}.json','w') as f:\n",
    "                json.dump(btq_response_json,f)\n",
    "    else:\n",
    "        assert LOAD_BT_RESPONSE == True\n",
    "        with open(f'responses/reponse_bt_{i+1}.json','r') as f:\n",
    "            btq_response_json = json.load(f)\n",
    "        #print(btq_response_json.keys())\n",
    "    pprint.pprint(btq_response_json)\n",
    "    print('----')\n",
    "    bt_xml = btq_response_json['Tree']\n",
    "    for k,v in btq_response_json.items():\n",
    "        if 'custom' in str.lower(k):\n",
    "            custom_node_requests.append(v)\n",
    "    with open(os.path.join(HUNAV_SIM_BT_FOLDER,f'LLMBT_{i}.xml'),'w') as f:\n",
    "        f.write(bt_xml)\n",
    "    print(f\"Wrote BT to LLMBT_{i}.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM for custom Nodes and Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_node_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(custom_node_requests)):\n",
    "    if QUERY_AUX:\n",
    "        ctnq = NodeQuery()\n",
    "        ctnq_full_prompt = ctnq.get_full_prompt(description = custom_node_requests[i])\n",
    "        payload = model.get_payload(content = ctnq_full_prompt)\n",
    "        response = model.get_response(messages = payload,format = 'json_object')\n",
    "        ctnq_response_json = json.loads(response)\n",
    "        if SAVE_AUX_RESPONSE:\n",
    "            with open('responses/reponse_aux.json','w') as f:\n",
    "                json.dump(ctnq_response_json,f)\n",
    "    else:\n",
    "        assert LOAD_AUX_RESPONSE == True\n",
    "        with open('responses/reponse_aux.json','r') as f:\n",
    "            ctnq_response_json = json.load(f)\n",
    "    with open(os.path.join('templates','extended_bt_functions.cpp'),'r') as f:\n",
    "        btf_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_functions.hpp'),'r') as f:\n",
    "        btf_hpp = f.read()\n",
    "\n",
    "    #register node in bt_node.cpp\n",
    "    with open(os.path.join('templates','extended_agent_manager.cpp'),'r') as f:\n",
    "        agm_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_agent_manager.hpp'),'r') as f:\n",
    "        agm_hpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_node.cpp'),'r') as f:\n",
    "        btn_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_node.hpp'),'r') as f:\n",
    "        btn_hpp = f.read()\n",
    "    \n",
    "    #Write functions to extended_bt_functions.cpp file\n",
    "    btf = ctnq_response_json['NODE_DEFINITION']\n",
    "    btf = btf.replace('BT::NodeStatus BTfunctions::','BT::NodeStatus BTfunctionsExt::')\n",
    "    btf_name = ctnq_response_json['NODE_NAME']\n",
    "    btf_type = ctnq_response_json['NODE_TYPE'].lower().capitalize()\n",
    "    btfn_name = btf_name[0].lower() + btf_name[1:]\n",
    "    btf_header = ctnq_response_json['NODE_HEADER']\n",
    "\n",
    "    btf_cpp = btf_cpp.replace('//<NEW FUNCTION>','//<NEW FUNCTION> \\n' + btf)\n",
    "    btf_hpp = btf_hpp.replace('//<NEW PUBLIC FUNCTION>','//<NEW PUBLIC FUNCTION> \\n' + btf_header)\n",
    "    \n",
    "    #register BT nodes in extended_bt_node.cpp\n",
    "    #   3 ports are available for each BT node:\n",
    "    #       simple_port: agent_id\n",
    "    #       visibleports: agent_id + distance\n",
    "    #       portsNav: agent_id + timestep\n",
    "    \n",
    "    node_register = f\"\"\"factory_.registerSimple{btf_type}(\"{ctnq_response_json['NODE_NAME']}\",std::bind(&BTfunctionsExt::{btfn_name},&btfunc_, _1),PORT);\"\"\"\n",
    "    if ctnq_response_json['PORTS_USED'] == ['agent_id','distance']:\n",
    "        node_register = node_register.replace('PORT','visibleports')\n",
    "    elif ctnq_response_json['PORTS_USED'] == ['agent_id','time_step']:\n",
    "        node_register = node_register.replace('PORT','portsNav')\n",
    "    elif ctnq_response_json['PORTS_USED'] == ['agent_id']:\n",
    "        node_register = node_register.replace('PORT','simple_port')\n",
    "    else:\n",
    "        node_register = node_register.replace(',PORT','')\n",
    "\n",
    "    btn_cpp = btn_cpp.replace('//<NEW NODE REGISTER>','//<NEW NODE REGISTER> \\n' + node_register)   \n",
    "\n",
    "    #add aux functions\n",
    "    for j,agmf in enumerate(ctnq_response_json['AUX_FUNCTIONS']):\n",
    "        agmf = agmf.replace('void AgentManager::','void AgentManagerExt::')\n",
    "        agm_cpp = agm_cpp.replace('//<NEW FUNCTION>','//<NEW FUNCTION> \\n' + agmf)\n",
    "        agm_hpp = agm_hpp.replace('//<NEW PUBLIC FUNCTION>','//<NEW PUBLIC FUNCTION> \\n' + ctnq_response_json['AUX_FUNCTION_HEADERS'][j])\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_bt_functions.cpp'),'w') as f:\n",
    "    f.writelines(btf_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_bt_functions.hpp'),'w') as f:\n",
    "    f.writelines(btf_hpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_bt_node.cpp'),'w') as f:\n",
    "    f.writelines(btn_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_bt_node.hpp'),'w') as f:\n",
    "    f.writelines(btn_hpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_agent_manager.cpp'),'w') as f:\n",
    "    f.writelines(agm_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_agent_manager.hpp'),'w') as f:\n",
    "    f.writelines(agm_hpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = subprocess.getstatusoutput(f' cd ~/catkin_ws && colcon build')\n",
    "if s[0] == 0:\n",
    "    print('Build Successful')\n",
    "else:\n",
    "    print('Build Failed')\n",
    "    print(s[1]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
