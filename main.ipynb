{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "from PIL import Image as PILImage\n",
    "from Prompts import *\n",
    "import pickle as pkl\n",
    "import yaml\n",
    "from utils.config import *\n",
    "from utils.models import *\n",
    "import copy\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpt model\n"
     ]
    }
   ],
   "source": [
    "model = Model(config = dict(\n",
    "    MODEL_NAME = MODEL\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('locations',LOCATION,'scene_graph.json'),'r') as f:\n",
    "    scene_graph = json.load(f)\n",
    "scQ = ScenarioQuery()\n",
    "coarse_scene_graph = utils.filter_scene_graph(scene_graph,'child')\n",
    "scQ_full_prompt = scQ.get_full_prompt(context=CONTEXT,\n",
    "    task=TASK,\n",
    "    graph=coarse_scene_graph\n",
    ")\n",
    "# print(scQ_txt_payload[1]['content'][0]['text'])\n",
    "# print(f\"\"\"Num tokens from main message: {utils.num_tokens_from_messages(scQ_txt_payload[1],MODEL)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Scenario Proposal from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a helpful assistant designed to output JSON'}, {'role': 'user', 'content': [{'type': 'text', 'text': ' Social navigation involves a robot navigating to a goal in the presence of humans and adhering to the following principles:\\n    Safety (P1): Ensure no harm comes to people or property by avoiding collisions and risky behaviors.\\n    Comfort (P2): Maintain personal space and move in ways that do not startle or distress individuals.\\n    Legibility (P3): Make the robot’ s actions predictable and understandable to those around it.\\n    Politeness (P4): Navigate with courtesy, often signaling intent and yielding way as needed.\\n    Social Competency (P5): Adhere to social norms and cultural expectations in shared environments. Adhering to Cultural norms is also very important and included within this.\\n    Understanding Other Agents (P6): Recognize and appropriately respond to the behaviors and intentions of others.\\n    Proactivity (P7): Anticipate and address potential social dilemmas or conflicts before they arise.\\n    Task Efficiency (P8): Complete the given task in a timely manner without sacrificing other social principles.\\nHowever, since its not possible to optimize all of these at the same time always, depending on the social situation and the task, the robot must favor some principles over the other. For example, when the robot is trying to deliver an emergency cart to a patient, P2 and P5 take less precedence over P1 and P8. Similarly, in a library, when a robot is delivering a book, P5, P2 and P1 are more important to optimize than P8.\\nSocial Navigation is a complex skill for a robot to accomplish and the appropriateness of the behavior of a robot is highly dependent on the task and the social context. Thus a robot’s social navigation capabilities must be thoroughly tested, and this is done by evaluating the robot’s behavior in a number of scenarios in a variety of contexts.\\n    \\nYou are a scenario designer. Your task is to generate scenarios to test the social navigation capabilities of a robot.\\nUser will provide a [Social context], a [Task] and the associated [Scene Graph] of the location. \\nYou must provide an appropriate [Scenario] to test the performance of the robot\\'s navigation algorithm, along with the corresponding [Expected behavior] of the robot based on a [Ranking] of the priorities of the Principles for the [Scenario]. You must also explain the [Reasoning] behind why its important to test the robot in this [Scenario] given the User input.\\nUse scene graph nodes when refering to locations. A Scene graph node is defined in the following format: id (name (optional)). For example, 1 (Bedroom), 2, 5 (Bathroom) etc.\\nAssume that the robot being tested is a simple navigation bot. It can also say the following phrases ONLY: [”WAIT”, “PROCEED”, “MAKE WAY”, “I AM HERE”].\\n\\nHuman agents in the scenario are overly simplified humans capable of doing the following ONLY:\\n    1. Navigating from one point to another\\n    2. Navigating from one point to another through specific waypoints\\n    3. Saying the following to the robot [\"WAIT\",\"PROCEED\",\"COME HERE\",\"GO AWAY\"]\\n    4. Gathering in groups of sizes ranging from 2-5. This includes Forming a group, joining a group, exiting a group. \\n    5. Intereacting with the robot, for example, approaching the robot, following the robot, blocking the robot, looking at the robot, walking along with the robot, leading the robot.\\n    \\n\\nYOU ADHERE TO THE FOLLOWING OUTPUT FORMAT STRICTLY. This is very important since your ouput will be used by a program later on. Do not provide any additional explanation. \\n    {\\n        \\'Scenario Description\\': < Subjective description of the scenario >,\\n         \\'Number of Humans\\': <Number of humans that are involved in the scenario>,\\n         \\'Trajectories\\':{\\n        \\'Robot\\': <comma separated sequence of Scene Graph nodes>,\\n        \\'Human 1\\': ....\\n        \\'Human 2\\': ...\\n                        },\\n    {\\'Behaviors\\':{\\n        \\'Human 1\\': <Description of behavior of Human 1 w.r.t the robot>,\\n        \\'Human 2\\':...,\\n    },\\n        \\n    \\'Expected Robot Behavior\\': <Describe the behavior expected from the robot>,\\n    \\'Principle Ranking\\': The order of importance for the 8 principles for this scenario,\\n    \\'Reasoning\\': <4-5 line description of why this scenario is relevant for the given input for testing>\\n    }\\n    \\n### Example:\\nUser:\\n[Social context]: Robot is a home assistant in a Singaporean old-age home and performs daily helpful duties for the residents\\n[Task]: Deliver coffee\\n[Scene Graph]:\\n{\\'nodes\\': [{\\'type\\': \\'room\\', \\'id\\': \\'1 (Bedroom)\\'}, {\\'type\\': \\'room\\', \\'id\\': \\'2 (Living Room)\\'}, {\\'type\\': \\'room\\', \\'id\\': \\'3 (Kitchen)\\'}, {\\'type\\': \\'connector\\', \\'id\\': \\'4 (Doorway)\\'}], \\'links\\': [\\'1 (Bedroom)<->2 (Living Room)\\', \\'2 (Living Room)<->4 (Doorway)\\', \\'3 (Kitchen)<->4 (Doorway)\\']}\\n\\nAssistant:\\n    {\\'Scenario Description\\': \"The robot is trying to deliver coffee from the 3 (Kitchen) to the 2 (Living Room) and encounters one of the elderly residents entering the 3 (Kitchen) from the 2 (Living Room) through the 4 (Doorway).\",\\n    \\'Number of Humans\\': 1,\\n    \\'Trajectories\\':{\\n        \\'Robot\\': [\"3 (Kitchen)\",\"4 (Doorway)\", \"2 (Living Room)\"],\\n        \\'Human 1\\': [\"Living Room(2)\",\"Doorway(4)\",\"3 (Kitchen)\"]\\n                    },\\n    \\'Behaviors\\':{\\n        \\'Human 1\\': \"Human has bad eyesight and cannot see the robot unless its very close. When the human sees the robot, they stop until the robot says something (for a maximum of 5 seconds) and then continue their activity.\"\\n        },\\n    \\'Expected Robot Behavior\\': \"The robot says \"I AM HERE\" to the resident. It waits for the resident to be well clear of the Doorway(4) before saying \"MOVING\" and going through the Doorway(4) to the 2 (Living Room) in a slow pace.\",\\n    \\'Principle Ranking\\': \"Safety (P1)>Social Competency (P5)>Understanding Other Agents (P6)>Legibility(P3)>Comfort(P2)>Politeness(P4)>Proactivity(P7)>Task Efficiency (P8)\",\\n    \\'Reasoning\\': \"It is important to evaluate the behavior of the robot when being confronted in close spaces by different types of humans. In an elderly home, there are likely elderly residents doing daily tasks who could be scared of the robot.\"\\n    }\\n###\\n    \\n\\nUSER:\\n[Social Context]:Robot is a delivery bot in a small warehouse. There are employees performing daily duties walking around the warehouse.\\n[Task]:Guide a human to a rack for inventory check.\\n[SCENE GRAPH]:{\\'nodes\\': [{\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [28, 152], \\'id\\': 1}, {\\'type\\': \\'CORNER\\', \\'pos\\': [22, 316], \\'id\\': 2}, {\\'type\\': \\'INTERSECTION\\', \\'pos\\': [190, 322], \\'id\\': 3}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [184, 162], \\'id\\': 4}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [107, 320], \\'id\\': 5}, {\\'type\\': \\'INTERSECTION\\', \\'pos\\': [500, 312], \\'id\\': 7}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [506, 159], \\'id\\': 8}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [656, 152], \\'id\\': 9}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [771, 158], \\'id\\': 10}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [884, 142], \\'id\\': 11}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [1006, 154], \\'id\\': 12}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [1116, 147], \\'id\\': 13}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [1248, 151], \\'id\\': 14}, {\\'type\\': \\'INTERSECTION\\', \\'pos\\': [662, 315], \\'id\\': 15}, {\\'type\\': \\'INTERSECTION\\', \\'pos\\': [779, 319], \\'id\\': 16}, {\\'type\\': \\'INTERSECTION\\', \\'pos\\': [892, 315], \\'id\\': 17}, {\\'type\\': \\'INTERSECTION\\', \\'pos\\': [1002, 308], \\'id\\': 18}, {\\'type\\': \\'INTERSECTION\\', \\'pos\\': [1122, 314], \\'id\\': 19}, {\\'type\\': \\'CORNER\\', \\'pos\\': [1216, 323], \\'id\\': 20}, {\\'type\\': \\'OPEN AREA\\', \\'pos\\': [688, 506], \\'id\\': 21}, {\\'type\\': \\'OPEN AREA\\', \\'pos\\': [872, 490], \\'id\\': 22}, {\\'type\\': \\'OPEN AREA\\', \\'pos\\': [491, 419], \\'id\\': 23}, {\\'type\\': \\'OPEN AREA\\', \\'pos\\': [276, 424], \\'id\\': 24}, {\\'type\\': \\'OPEN AREA\\', \\'pos\\': [1080, 435], \\'id\\': 25}, {\\'type\\': \\'CORNER\\', \\'pos\\': [76, 671], \\'id\\': 26}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [243, 656], \\'id\\': 27}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [403, 662], \\'id\\': 28}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [594, 659], \\'id\\': 29}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [760, 659], \\'id\\': 30}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [935, 655], \\'id\\': 31}, {\\'type\\': \\'CORNER\\', \\'pos\\': [1206, 654], \\'id\\': 32}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [1216, 528], \\'id\\': 33}, {\\'type\\': \\'PASSAGEWAY\\', \\'pos\\': [74, 786], \\'id\\': 34}], \\'links\\': [\\'1<->2\\', \\'2<->5\\', \\'3<->5\\', \\'3<->4\\', \\'3<->24\\', \\'7<->8\\', \\'7<->23\\', \\'7<->15\\', \\'9<->15\\', \\'10<->16\\', \\'11<->17\\', \\'12<->18\\', \\'13<->19\\', \\'14<->20\\', \\'15<->16\\', \\'15<->21\\', \\'16<->17\\', \\'16<->22\\', \\'16<->21\\', \\'17<->18\\', \\'17<->22\\', \\'17<->25\\', \\'18<->19\\', \\'18<->25\\', \\'19<->20\\', \\'19<->25\\', \\'20<->25\\', \\'21<->23\\', \\'21<->22\\', \\'21<->29\\', \\'21<->30\\', \\'22<->25\\', \\'22<->30\\', \\'22<->31\\', \\'23<->24\\', \\'26<->27\\', \\'26<->34\\', \\'27<->28\\', \\'28<->29\\', \\'29<->30\\', \\'30<->31\\', \\'31<->32\\', \\'32<->33\\']}\\nAssistant:\\n'}]}]\n",
      "detected text\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "payload = model.get_payload(content = scQ_full_prompt)\n",
    "print(payload)\n",
    "total_tokens = 0\n",
    "for msg in payload:\n",
    "    total_tokens += utils.num_tokens_from_messages(msg,\"gpt-4\")\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Scenario Description', 'Number of Humans', 'Trajectories', 'Behaviors', 'Expected Robot Behavior', 'Principle Ranking', 'Reasoning'])\n",
      "{'Behaviors': {'Human 1': 'Follows the robot from 4 to 13 for inventory check.',\n",
      "               'Human 2': 'Crosses the path of the robot and Human 1 at 3.',\n",
      "               'Human 3': 'Moves in the same direction as the robot between 7 '\n",
      "                          'and 8 but faster, potentially overtaking.'},\n",
      " 'Expected Robot Behavior': 'The robot leads Human 1 from 4 to 13, slowing '\n",
      "                            'down at intersections to allow Human 2 to cross '\n",
      "                            'first at 3 and then proceeds. If approached by '\n",
      "                            'Human 3, the robot maintains its pace without '\n",
      "                            \"impeding the human's faster movement. The robot \"\n",
      "                            'communicates its moves at intersections with '\n",
      "                            \"'PROCEED' and uses 'MAKE WAY' when encountering \"\n",
      "                            'blocked paths.',\n",
      " 'Number of Humans': 3,\n",
      " 'Principle Ranking': 'Understanding Other Agents (P6)>Safety (P1)>Legibility '\n",
      "                      '(P3)>Politeness (P4)> Comfort (P2)>Social Competency '\n",
      "                      '(P5)>Proactivity (P7)>Task Efficiency (P8)',\n",
      " 'Reasoning': 'This scenario tests the robot’s ability to interact with '\n",
      "              'multiple humans in a dynamic environment where paths cross and '\n",
      "              'to communicate its intent clearly at intersections while '\n",
      "              'maintaining safety and efficient flow of human and robot '\n",
      "              'traffic in a warehouse setting.',\n",
      " 'Scenario Description': 'A human employee needs guidance to reach the rack '\n",
      "                         'located at the 13 (PASSAGEWAY) for an inventory '\n",
      "                         'check. The robot is supposed to meet the human at 4 '\n",
      "                         '(PASSAGEWAY) and guide them through a busy warehouse '\n",
      "                         'with other employees crossing their path at 3 '\n",
      "                         '(INTERSECTION) and 9 (PASSAGEWAY).',\n",
      " 'Trajectories': {'Human 1': ['4', '3', '5', '7', '9', '15', '13'],\n",
      "                  'Human 2': ['1', '2', '5', '3'],\n",
      "                  'Human 3': ['7', '8'],\n",
      "                  'Robot': ['4', '3', '5', '7', '9', '15', '13']}}\n"
     ]
    }
   ],
   "source": [
    "if QUERY_SC:\n",
    "    payload = model.get_payload(content = scQ_full_prompt)\n",
    "    response = model.get_response(messages = payload,format = \"json_object\")\n",
    "    scq_response_json = json.loads(response)\n",
    "    if SAVE_SC_RESPONSE:\n",
    "        with open('responses/reponse_sc.json','w') as f:\n",
    "            json.dump(scq_response_json,f)\n",
    "else:\n",
    "    assert LOAD_SC_RESPONSE == True\n",
    "    with open('responses/reponse_sc.json','r') as f:\n",
    "        scq_response_json = json.load(f)\n",
    "print(scq_response_json.keys())\n",
    "pprint.pprint(scq_response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HANDCRAFTED_SCENARIO:\n",
    "    scenario_desc = scenario_desc_hc\n",
    "    num_humans = num_humans_hc\n",
    "    traj_desc = traj_desc_hc\n",
    "    behav_desc = behav_desc_hc\n",
    "else:\n",
    "    scenario_desc = scq_response_json['Scenario Description']\n",
    "    num_humans = scq_response_json['Number of Humans']\n",
    "    traj_desc = scq_response_json['Trajectories']\n",
    "    behav_desc = scq_response_json['Behaviors']\n",
    "\n",
    "# Assume the output is formatted correctly\n",
    "expected_robot_behav_desc = scq_response_json['Expected Robot Behavior'] \n",
    "pranking_desc = scq_response_json['Principle Ranking'] \n",
    "reasoning_desc = scq_response_json['Reasoning'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarse_locations:['4', '3', '5', '7', '9', '15', '13', '1', '2', '8']\n"
     ]
    }
   ],
   "source": [
    "coarse_locations = []\n",
    "for k,v in traj_desc.items():\n",
    "    for loc in v:\n",
    "        if loc not in coarse_locations:\n",
    "            coarse_locations.append(loc)\n",
    "print(f'coarse_locations:{coarse_locations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250, 250):255\n",
      "(250, 250):255\n",
      "(232, 250):255\n",
      "(250, 250):255\n",
      "(250, 250):255\n",
      "(250, 250):255\n",
      "(250, 250):255\n",
      "(153, 250):255\n",
      "(147, 250):255\n",
      "(250, 250):255\n",
      "(1285, 859):1105\n",
      "Total cost for images: $0.03655\n"
     ]
    }
   ],
   "source": [
    "coarse_loc_imgs = []\n",
    "cost = 0\n",
    "for img_name in coarse_locations:\n",
    "    img_path = os.path.join('locations',LOCATION,f'{img_name}.png')\n",
    "    encoded_img,img_cost = utils.load_imgs_for_prompt(img_path)\n",
    "    coarse_loc_imgs.append(encoded_img)\n",
    "    cost+=img_cost\n",
    "\n",
    "img_path = os.path.join('locations',LOCATION,'scene_graph.png')\n",
    "encoded_img,img_cost = utils.load_imgs_for_prompt(img_path)\n",
    "coarse_loc_imgs.append(encoded_img)\n",
    "cost+=img_cost\n",
    "print(f\"Total cost for images: ${(cost / 1000.0)*0.01}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Locations Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "flocationQ = FLocationQuery(\n",
    "    loc_imgs = coarse_loc_imgs\n",
    ")\n",
    "flq_full_prompt = flocationQ.get_full_prompt(\n",
    "    scene_graph = str(scene_graph),\n",
    "    num_humans = num_humans,\n",
    "    traj_desc = traj_desc,\n",
    "    sc_desc = scenario_desc\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Fine Locations from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Trajectory', 'Group'])\n",
      "{'Group': {'Human 1': -1, 'Human 2': 1, 'Human 3': 1},\n",
      " 'Trajectory': {'Human 1': '4B,3A,5A,7A,9A,15A,13B',\n",
      "                'Human 2': '1A,2A,5B,3B',\n",
      "                'Human 3': '7B,8B',\n",
      "                'Robot': '4A,3B,5A,7A,9B,15B,13A'}}\n"
     ]
    }
   ],
   "source": [
    "if QUERY_LOC:\n",
    "    payload = model.get_payload(content = flq_full_prompt)\n",
    "    response = model.get_response(messages = payload,format = 'json_object')\n",
    "    flq_response_json = json.loads(response)\n",
    "    if SAVE_LOC_RESPONSE:\n",
    "        with open('responses/reponse_loc.json','w') as f:\n",
    "            json.dump(flq_response_json,f)\n",
    "else:\n",
    "    assert LOAD_LOC_RESPONSE == True\n",
    "    with open('responses/reponse_loc.json','r') as f:\n",
    "        flq_response_json = json.load(f)\n",
    "print(flq_response_json.keys())\n",
    "pprint.pprint(flq_response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Human 1': ['4B', '3A', '5A', '7A', '9A', '15A', '13B'], 'Human 2': ['1A', '2A', '5B', '3B'], 'Human 3': ['7B', '8B'], 'Robot': ['4A', '3B', '5A', '7A', '9B', '15B', '13A']}\n"
     ]
    }
   ],
   "source": [
    "trajectories = {}\n",
    "for k,v in flq_response_json['Trajectory'].items():\n",
    "    trajectories[k] = v.split(',')\n",
    "print(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard coded for current small warehouse map scene\n",
    "def pix2world(px):\n",
    "    return [(px[0]/3.0) * 0.050000 + -7.000 ,-1*((px[1]/3.0) * 0.050000 + -10.500000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_positions = {}\n",
    "for node in scene_graph['nodes']:\n",
    "    nodes_positions[node['id']] = pix2world(node['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Human 1': [[-3.9, 9.233333333333334], [-5.266666666666667, 5.3], [-6.45, 5.166666666666666], [-0.06666666666666643, 5.283333333333333], [3.9000000000000004, 9.466666666666667], [2.3000000000000007, 5.166666666666666], [11.533333333333335, 9.566666666666666]], 'Human 2': [[-6.483333333333333, 9.1], [-6.583333333333333, 6.466666666666666], [-4.166666666666666, 5.166666666666666], [-3.883333333333333, 6.666666666666666]], 'Human 3': [[2.950000000000001, 5.199999999999999], [1.4333333333333336, 9.233333333333334]], 'Robot': [[-3.9166666666666665, 6.25], [-3.883333333333333, 6.666666666666666], [-6.45, 5.166666666666666], [-0.06666666666666643, 5.283333333333333], [4.0, 6.816666666666666], [4.0, 6.85], [11.766666666666666, 6.566666666666666]]}\n"
     ]
    }
   ],
   "source": [
    "trajectories_world_coords = {}\n",
    "for k,v in trajectories.items():\n",
    "    trajectories_world_coords[k] = []\n",
    "    for loc in v:\n",
    "        trajectories_world_coords[k].append(nodes_positions[loc])\n",
    "print(trajectories_world_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add fine locations to sim yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_yaml = {'hunav_loader': {'ros__parameters': {'map': LOCATION,\n",
    "   'publish_people': True,\n",
    "   'agents': []}}}\n",
    "blank_human = {'id': None,\n",
    "    'skin': 0,\n",
    "    'behavior': 0,\n",
    "    'group_id': -1,\n",
    "    'max_vel': 1.5,\n",
    "    'radius': 0.4,\n",
    "    'init_pose': {'x': None, 'y': None, 'z': 1.25, 'h': 0.0},\n",
    "    'goal_radius': 0.3,\n",
    "    'cyclic_goals': False,\n",
    "    'goals': [],\n",
    "    }\n",
    "agents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_humans):\n",
    "    agents_yaml['hunav_loader']['ros__parameters']['agents'].append(f'agent{i}')\n",
    "    agents[f'agent{i}'] = copy.deepcopy(blank_human)\n",
    "    agents[f'agent{i}']['id'] = i\n",
    "    agents[f'agent{i}']['behavior'] = 7+i\n",
    "    for j,g in enumerate(trajectories_world_coords[f'Human {i+1}']):\n",
    "        if j == 0:\n",
    "            agents[f'agent{i}']['init_pose'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'z':1.25,\n",
    "                'h':0.0,\n",
    "            } \n",
    "        else:\n",
    "            agents[f'agent{i}']['goals'].append(f'g{j}')\n",
    "            agents[f'agent{i}'][f'g{j}'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'h':1.25\n",
    "            }\n",
    "agents_yaml['hunav_loader']['ros__parameters'].update(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HUNAV_SIM_AGENTS_FILE,'w') as f:\n",
    "    yaml.dump(agents_yaml,f)\n",
    "\n",
    "with open(os.path.join(HUNAV_GAZEBO_WRAPPER_DIR,'config','robot.yaml'),'w') as f:\n",
    "    yaml.dump({\n",
    "    'x_pose': trajectories_world_coords['Robot'][0][0],\n",
    "    'y_pose': trajectories_world_coords['Robot'][0][1]\n",
    "},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM for BT for humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Human 1': 'Follows the robot from 4 to 13 for inventory check.',\n",
       " 'Human 2': 'Crosses the path of the robot and Human 1 at 3.',\n",
       " 'Human 3': 'Moves in the same direction as the robot between 7 and 8 but faster, potentially overtaking.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behav_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_desc['Human 1'] = 'Follows the robot from location 1 to location 2 for inventory check.'\n",
    "behav_desc['Human 2'] = 'Crosses the path of the robot and Human 1 at location 3.'\n",
    "behav_desc['Human 3'] = 'Moves in the same direction as the robot between location 4 and location 5 but faster, potentially overtaking.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follows the robot from location 1 to location 2 for inventory check.\n",
      "{'Custom Action 1': 'Design an Action node called FollowToLocation(agent_id, '\n",
      "                    \"start, end) that initiates the agent's movement following \"\n",
      "                    'the robot from the start location to the end location for '\n",
      "                    'inventory. It should return RUNNING while the agent is '\n",
      "                    'moving, SUCCESS once the agent reaches the end location, '\n",
      "                    'and FAILURE if it cannot execute the action.',\n",
      " 'Custom Condition 1': 'Design a Condition node called IsAtLocation(agent_id, '\n",
      "                       'location) that checks if the agent is at the specified '\n",
      "                       'location. It returns SUCCESS if the agent is at the '\n",
      "                       'given location, otherwise it returns FAILURE.',\n",
      " 'Reasoning': 'This behavior requires a custom node to implement the following '\n",
      "              'behavior since it is not explicitly provided by existing nodes. '\n",
      "              'The human needs to follow the robot from one location to '\n",
      "              'another. If the starting and ending locations are pre-defined '\n",
      "              'and known in the design of the behavior tree, we would need to '\n",
      "              'combine the RegularNav action that moves towards the goal with '\n",
      "              'a custom condition to check if the human is at location 1 and '\n",
      "              'then begin following to location 2. However, without concrete '\n",
      "              \"'check location' conditions or 'follow to location' actions, a \"\n",
      "              'custom node will be required. The following behavior will '\n",
      "              'utilize placeholders for these custom nodes.',\n",
      " 'Tree': \"<root main_tree_to_execute = 'FollowForInventoryTree'>\\n\"\n",
      "         \"    <BehaviorTree ID='FollowForInventoryTree'>\\n\"\n",
      "         \"        <Sequence name='InventoryCheckSequence'>\\n\"\n",
      "         \"            <IsAtLocation agent_id='{id}' location='1'/>\\n\"\n",
      "         \"            <FollowToLocation agent_id='{id}' start='1' end='2' />\\n\"\n",
      "         '        </Sequence>\\n'\n",
      "         '    </BehaviorTree>\\n'\n",
      "         '</root>'}\n",
      "----\n",
      "Wrote BT to LLMBT_0.xml\n",
      "Crosses the path of the robot and Human 1 at location 3.\n",
      "{'Custom Action 1': 'Design an Action node called CrossAtPath(agent_id, '\n",
      "                    'location) that moves the agent to cross the path of the '\n",
      "                    'robot and Human 1 at the specified location. Returns '\n",
      "                    'SUCCESS once the path is crossed, FAILURE if unable to '\n",
      "                    'cross for some reason, and RUNNING while the agent is in '\n",
      "                    'the process of crossing.',\n",
      " 'Reasoning': 'To achieve the desired behavior of crossing the path of the '\n",
      "              'robot and Human 1, a custom action node that handles path '\n",
      "              \"crossing at a specific location is necessary because there's no \"\n",
      "              'existing action with this functionality. The rest of the '\n",
      "              'behavior can use existing nodes.',\n",
      " 'Tree': '<root main_tree_to_execute = \"CrossPathTree\">\\n'\n",
      "         '    <BehaviorTree ID=\"CrossPathTree\">\\n'\n",
      "         '        <Sequence name=\"CrossPathSequence\">\\n'\n",
      "         '            <CrossAtPath agent_id=\"{id}\" location=\"3\" />\\n'\n",
      "         '        </Sequence>\\n'\n",
      "         '    </BehaviorTree>\\n'\n",
      "         '</root>'}\n",
      "----\n",
      "Wrote BT to LLMBT_1.xml\n",
      "Moves in the same direction as the robot between location 4 and location 5 but faster, potentially overtaking.\n"
     ]
    }
   ],
   "source": [
    "custom_node_requests = []\n",
    "for i in range(num_humans):\n",
    "    btq = BTQuery()\n",
    "    if QUERY_BT:\n",
    "        print(behav_desc[f'Human {i+1}'])\n",
    "        btq_full_prompt = btq.get_full_prompt(behavior = behav_desc[f'Human {i+1}'])\n",
    "        payload = model.get_payload(content = btq_full_prompt)\n",
    "        response = model.get_response(messages = payload,format = 'json_object')\n",
    "        btq_response_json = json.loads(response)\n",
    "        if SAVE_BT_RESPONSE:\n",
    "            with open(f'responses/reponse_bt_{i+1}.json','w') as f:\n",
    "                json.dump(btq_response_json,f)\n",
    "    else:\n",
    "        assert LOAD_BT_RESPONSE == True\n",
    "        with open(f'responses/reponse_bt_{i+1}.json','r') as f:\n",
    "            btq_response_json = json.load(f)\n",
    "        #print(btq_response_json.keys())\n",
    "    pprint.pprint(btq_response_json)\n",
    "    print('----')\n",
    "    bt_xml = btq_response_json['Tree']\n",
    "    for k,v in btq_response_json.items():\n",
    "        if 'custom' in str.lower(k):\n",
    "            custom_node_requests.append(v)\n",
    "    with open(os.path.join(HUNAV_SIM_BT_FOLDER,f'LLMBT_{i}.xml'),'w') as f:\n",
    "        f.write(bt_xml)\n",
    "    print(f\"Wrote BT to LLMBT_{i}.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM for custom Nodes and Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(custom_node_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(custom_node_requests)):\n",
    "    if QUERY_AUX:\n",
    "        ctnq = NodeQuery()\n",
    "        ctnq_full_prompt = ctnq.get_full_prompt(description = custom_node_requests[i])\n",
    "        payload = model.get_payload(content = ctnq_full_prompt)\n",
    "        response = model.get_response(messages = payload,format = 'json_object')\n",
    "        ctnq_response_json = json.loads(response)\n",
    "        if SAVE_AUX_RESPONSE:\n",
    "            with open('responses/reponse_aux.json','w') as f:\n",
    "                json.dump(ctnq_response_json,f)\n",
    "    else:\n",
    "        assert LOAD_AUX_RESPONSE == True\n",
    "        with open('responses/reponse_aux.json','r') as f:\n",
    "            ctnq_response_json = json.load(f)\n",
    "    with open(os.path.join('templates','extended_bt_functions.cpp'),'r') as f:\n",
    "        btf_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_functions.hpp'),'r') as f:\n",
    "        btf_hpp = f.read()\n",
    "\n",
    "    #register node in bt_node.cpp\n",
    "    with open(os.path.join('templates','extended_agent_manager.cpp'),'r') as f:\n",
    "        agm_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_agent_manager.hpp'),'r') as f:\n",
    "        agm_hpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_node.cpp'),'r') as f:\n",
    "        btn_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_node.hpp'),'r') as f:\n",
    "        btn_hpp = f.read()\n",
    "    \n",
    "    #Write functions to extended_bt_functions.cpp file\n",
    "    btf = ctnq_response_json['NODE_DEFINITION']\n",
    "    btf = btf.replace('BT::NodeStatus BTfunctions::','BT::NodeStatus BTfunctionsExt::')\n",
    "    btf_name = ctnq_response_json['NODE_NAME']\n",
    "    btf_type = ctnq_response_json['NODE_TYPE'].lower().capitalize()\n",
    "    btfn_name = btf_name[0].lower() + btf_name[1:]\n",
    "    btf_header = ctnq_response_json['NODE_HEADER']\n",
    "\n",
    "    btf_cpp = btf_cpp.replace('//<NEW FUNCTION>','//<NEW FUNCTION> \\n' + btf)\n",
    "    btf_hpp = btf_hpp.replace('//<NEW PUBLIC FUNCTION>','//<NEW PUBLIC FUNCTION> \\n' + btf_header)\n",
    "    \n",
    "    #register BT nodes in extended_bt_node.cpp\n",
    "    #   3 ports are available for each BT node:\n",
    "    #       simple_port: agent_id\n",
    "    #       visibleports: agent_id + distance\n",
    "    #       portsNav: agent_id + timestep\n",
    "    \n",
    "    node_register = f\"\"\"factory_.registerSimple{btf_type}(\"{ctnq_response_json['NODE_NAME']}\",std::bind(&BTfunctionsExt::{btfn_name},&btfunc_, _1),PORT);\"\"\"\n",
    "    if ctnq_response_json['PORTS_USED'] == ['agent_id','distance']:\n",
    "        node_register = node_register.replace('PORT','visibleports')\n",
    "    elif ctnq_response_json['PORTS_USED'] == ['agent_id','time_step']:\n",
    "        node_register = node_register.replace('PORT','portsNav')\n",
    "    elif ctnq_response_json['PORTS_USED'] == ['agent_id']:\n",
    "        node_register = node_register.replace('PORT','simple_port')\n",
    "    else:\n",
    "        node_register = node_register.replace(',PORT','')\n",
    "\n",
    "    btn_cpp = btn_cpp.replace('//<NEW NODE REGISTER>','//<NEW NODE REGISTER> \\n' + node_register)   \n",
    "\n",
    "    #add aux functions\n",
    "    for j,agmf in enumerate(ctnq_response_json['AUX_FUNCTIONS']):\n",
    "        agmf = agmf.replace('void AgentManager::','void AgentManagerExt::')\n",
    "        agm_cpp = agm_cpp.replace('//<NEW FUNCTION>','//<NEW FUNCTION> \\n' + agmf)\n",
    "        agm_hpp = agm_hpp.replace('//<NEW PUBLIC FUNCTION>','//<NEW PUBLIC FUNCTION> \\n' + ctnq_response_json['AUX_FUNCTION_HEADERS'][j])\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_bt_functions.cpp'),'w') as f:\n",
    "    f.writelines(btf_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_bt_functions.hpp'),'w') as f:\n",
    "    f.writelines(btf_hpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_bt_node.cpp'),'w') as f:\n",
    "    f.writelines(btn_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_bt_node.hpp'),'w') as f:\n",
    "    f.writelines(btn_hpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_agent_manager.cpp'),'w') as f:\n",
    "    f.writelines(agm_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_agent_manager.hpp'),'w') as f:\n",
    "    f.writelines(agm_hpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = subprocess.getstatusoutput(f' cd ~/catkin_ws && colcon build')\n",
    "if s[0] == 0:\n",
    "    print('Build Successful')\n",
    "else:\n",
    "    print('Build Failed')\n",
    "    print(s[1]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
