{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "from Prompts import *\n",
    "import pickle as pkl\n",
    "import yaml\n",
    "from utils.config import *\n",
    "from utils.models import *\n",
    "import copy\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpt model\n"
     ]
    }
   ],
   "source": [
    "model = Model(config = dict(\n",
    "    MODEL_NAME = MODEL\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('locations',LOCATION,'scene_graph.json'),'r') as f:\n",
    "    scene_graph = json.load(f)\n",
    "scQ = ScenarioQuery()\n",
    "scQ_full_prompt = scQ.get_full_prompt(context=CONTEXT,\n",
    "    task=TASK,\n",
    "    graph=scene_graph\n",
    ")\n",
    "# print(scQ_txt_payload[1]['content'][0]['text'])\n",
    "# print(f\"\"\"Num tokens from main message: {utils.num_tokens_from_messages(scQ_txt_payload[1],MODEL)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'type': 'narrow passage', 'id': 'Aisle1 (1)'},\n",
       "  {'type': 'intersection', 'id': 'Aisle2 (2)'},\n",
       "  {'type': 'open space', 'id': 'Aisle3 (3)'},\n",
       "  {'type': 'wide passage', 'id': 'Aisle (4)'},\n",
       "  {'type': 'open space', 'id': 'Hall (5)'},\n",
       "  {'type': 'narrow passage', 'id': 'Aisle6 (6)'},\n",
       "  {'type': 'narrow passage', 'id': 'Aisle7 (7)'}],\n",
       " 'links': ['Aisle1 (1)<->Hall (5)',\n",
       "  'Aisle2 (2)<->Aisle3 (3)',\n",
       "  'Aisle2 (2)<->Hall (5)',\n",
       "  'Aisle3 (3)<->Hall (5)',\n",
       "  'Aisle (4)<->Hall (5)',\n",
       "  'Hall (5)<->Aisle6 (6)',\n",
       "  'Hall (5)<->Aisle7 (7)']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Scenario Proposal from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a helpful assistant designed to output JSON'},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': ' Social navigation involves a robot navigating to a goal in the presence of humans and adhering to the following principles:\\n    Safety (P1): Ensure no harm comes to people or property by avoiding collisions and risky behaviors.\\n    Comfort (P2): Maintain personal space and move in ways that do not startle or distress individuals.\\n    Legibility (P3): Make the robot’ s actions predictable and understandable to those around it.\\n    Politeness (P4): Navigate with courtesy, often signaling intent and yielding way as needed.\\n    Social Competency (P5): Adhere to social norms and cultural expectations in shared environments. Adhering to Cultural norms is also very important and included within this.\\n    Understanding Other Agents (P6): Recognize and appropriately respond to the behaviors and intentions of others.\\n    Proactivity (P7): Anticipate and address potential social dilemmas or conflicts before they arise.\\n    Task Efficiency (P8): Complete the given task in a timely manner without sacrificing other social principles.\\nHowever, since its not possible to optimize all of these at the same time always, depending on the social situation and the task, the robot must favor some principles over the other. For example, when the robot is trying to deliver an emergency cart to a patient, P2 and P5 take less precedence over P1 and P8. Similarly, in a library, when a robot is delivering a book, P5, P2 and P1 are more important to optimize than P8.\\nSocial Navigation is a complex skill for a robot to accomplish and the appropriateness of the behavior of a robot is highly dependent on the task and the social context. Thus a robot’s social navigation capabilities must be thoroughly tested, and this is done by evaluating the robot’s behavior in a number of scenarios in a variety of contexts.\\n    \\nYou are a scenario designer. Your task is to generate scenarios to test the social navigation capabilities of a robot.\\nUser will provide a [Social context], a [Task] and the associated [Scene Graph] of the location. \\nYou must provide an appropriate [Scenario] to test the performance of the robot\\'s navigation algorithm, along with the corresponding [Expected behavior] of the robot based on a [Ranking] of the priorities of the Principles for the [Scenario]. You must also explain the [Reasoning] behind why its important to test the robot in this [Scenario] given the User input.\\nWhen refering to locations in the scenario, always refer to \\nAssume that the robot being tested is a simple navigation bot. It can also say the following phrases ONLY: [”WAIT”, “PROCEED”, “MAKE WAY”, “I AM HERE”,\"MOVING\"].\\n\\nHuman agents in the scenario are overly simplified humans capable of doing the following ONLY:\\n    1. Navigating from one point to another\\n    2. Navigating from one point to another through specific waypoints\\n    3. Saying the following to the robot [\"WAIT\",\"PROCEED\",\"COME HERE\",\"GO AWAY\"]\\n    4. Gathering in groups of sizes ranging from 2-5. This includes Forming a group, joining a group, exiting a group. \\n    5. Intereacting with the robot, for example, approaching the robot, following the robot, blocking the robot, looking at the robot, walking along with the robot, leading the robot.\\n    \\n\\nYOU ADHERE TO THE FOLLOWING OUTPUT FORMAT STRICTLY. This is very important since your ouput will be used by a program later on. Do not provide any additional explanation. \\n    {\\n        \\'Scenario Description\\': < Subjective description of the scenario >,\\n         \\'Number of Humans\\': <Number of humans that are involved in the scenario>,\\n         \\'Trajectories\\':{\\n        \\'Robot\\': <Sequence from the set: [Scene Graph nodes]>,\\n        \\'Human 1\\': <Sequence from the set: [Scene Graph nodes]>. The human can also be made to follow the robot by ending the sequence with \"CURRENT_ROBOT_LOCATION\",\\n        \\'Human 2\\': ...\\n                        },\\n    {\\'Behaviors\\':{\\n        \\'Human 1\\': <Description of behavior of Human 1 w.r.t the robot>,\\n        \\'Human 2\\':...,\\n    },\\n        \\n    \\'Expected Robot Behavior\\': <Describe the behavior expected from the robot>,\\n    \\'Principle Ranking\\': The order of importance for the 8 principles for this scenario,\\n    \\'Reasoning\\': <4-5 line description of why this scenario is relevant for the given input for testing>\\n    }\\n    \\n### Example:\\nUser:\\n[Social context]: Robot is a home assistant in a Singaporean old-age home and performs daily helpful duties for the residents\\n[Task]: Deliver coffee\\n[Scene Graph]:\\n{\\'nodes\\': [{\\'type\\': \\'room\\', \\'id\\': \\'Bedroom (1)\\'}, {\\'type\\': \\'room\\', \\'id\\': \\'Living Room (2)\\'}, {\\'type\\': \\'room\\', \\'id\\': \\'Kitchen (3)\\'}, {\\'type\\': \\'connector\\', \\'id\\': \\'Doorway (4)\\'}], \\'links\\': [\\'Bedroom (1)<->Living Room (2)\\', \\'Living Room (2)<->Doorway (4)\\', \\'Kitchen (3)<->Doorway (4)\\']}\\n\\nAssistant:\\n    {\\'Scenario Description\\': \"The robot is trying to deliver coffee from the Kitchen (3) to the Living Room (2) and encounters one of the elderly residents entering the Kitchen (3) from the Living room (2) through the Doorway (4).\",\\n    \\'Number of Humans\\': 1,\\n    \\'Trajectories\\':{\\n        \\'Robot\\': \"Kitchen (3) -> Doorway (4) -> Living Room (2)\",\\n        \\'Human 1\\': \"Living Room(2)-> Doorway(4) -> Kitchen (3)\"\\n                    },\\n    \\'Behaviors\\':{\\n        \\'Human 1\\': \"Human has bad eyesight and cannot see the robot unless its very close. When the human sees the robot, they stop until the robot says something (for a maximum of 5 seconds) and then continue their activity.\"\\n        },\\n    \\'Expected Robot Behavior\\': \"The robot says \"I AM HERE\" to the resident. It waits for the resident to be well clear of the Doorway(4) before saying \"MOVING\" and going through the Doorway(4) to the Living room (2) in a slow pace.\",\\n    \\'Principle Ranking\\': \"Safety (P1)>Social Competency (P5)>Understanding Other Agents (P6)>Legibility(P3)>Comfort(P2)>Politeness(P4)>Proactivity(P7)>Task Efficiency (P8)\",\\n    \\'Reasoning\\': \"It is important to evaluate the behavior of the robot when being confronted in close spaces by different types of humans. In an elderly home, there are likely elderly residents doing daily tasks who could be scared of the robot.\"\\n    }\\n###\\n    \\n\\nUSER:\\n[Social Context]:Robot is a delivery bot in a small japanese warehouse. There are employees performing daily duties walking around the warehouse.\\n[Task]:Navigate to a specific rack and perform inventory check.\\n[SCENE GRAPH]:{\\'nodes\\': [{\\'type\\': \\'narrow passage\\', \\'id\\': \\'Aisle1 (1)\\'}, {\\'type\\': \\'intersection\\', \\'id\\': \\'Aisle2 (2)\\'}, {\\'type\\': \\'open space\\', \\'id\\': \\'Aisle3 (3)\\'}, {\\'type\\': \\'wide passage\\', \\'id\\': \\'Aisle (4)\\'}, {\\'type\\': \\'open space\\', \\'id\\': \\'Hall (5)\\'}, {\\'type\\': \\'narrow passage\\', \\'id\\': \\'Aisle6 (6)\\'}, {\\'type\\': \\'narrow passage\\', \\'id\\': \\'Aisle7 (7)\\'}], \\'links\\': [\\'Aisle1 (1)<->Hall (5)\\', \\'Aisle2 (2)<->Aisle3 (3)\\', \\'Aisle2 (2)<->Hall (5)\\', \\'Aisle3 (3)<->Hall (5)\\', \\'Aisle (4)<->Hall (5)\\', \\'Hall (5)<->Aisle6 (6)\\', \\'Hall (5)<->Aisle7 (7)\\']}\\nAssistant:\\n'}]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_payload(content = scQ_full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Scenario Description', 'Number of Humans', 'Trajectories', 'Behaviors', 'Expected Robot Behavior', 'Principle Ranking', 'Reasoning'])\n",
      "{'Behaviors': {'Human 1': 'Employee moves swiftly through the intersection but '\n",
      "                          'acknowledges the presence of the robot by making '\n",
      "                          'brief eye contact.'},\n",
      " 'Expected Robot Behavior': 'The robot waits at the intersection Aisle2 (2) '\n",
      "                            \"and says 'MAKE WAY' to the employee. Once the \"\n",
      "                            'employee has passed, the robot proceeds to Aisle7 '\n",
      "                            '(7) at a steady pace.',\n",
      " 'Number of Humans': 1,\n",
      " 'Principle Ranking': 'Safety (P1) > Politeness (P4) > Task Efficiency (P8) > '\n",
      "                      'Social Competency (P5) > Legibility (P3) > Comfort (P2) '\n",
      "                      '> Understanding Other Agents (P6) > Proactivity (P7)',\n",
      " 'Reasoning': \"This scenario tests the robot's ability to interact with \"\n",
      "              'employees in a busy warehouse environment where quick movements '\n",
      "              'and awareness are crucial. Prioritizing Safety and Politeness '\n",
      "              'ensures smooth navigation and minimal disruption to the '\n",
      "              \"employees' tasks.\",\n",
      " 'Scenario Description': 'The robot is navigating from Aisle1 (1) to Aisle7 '\n",
      "                         '(7) to perform an inventory check. An employee is '\n",
      "                         'walking from Aisle6 (6) to Hall (5) and crosses '\n",
      "                         'paths with the robot at the intersection Aisle2 (2).',\n",
      " 'Trajectories': {'Human 1': 'Aisle6 (6) -> Hall (5)',\n",
      "                  'Robot': 'Aisle1 (1) -> Hall (5) -> Aisle7 (7)'}}\n"
     ]
    }
   ],
   "source": [
    "if QUERY_SC:\n",
    "    payload = model.get_payload(content = scQ_full_prompt)\n",
    "    response = model.get_response(messages = payload,format = \"json_object\")\n",
    "    scq_response_json = json.loads(response)\n",
    "    if SAVE_SC_RESPONSE:\n",
    "        with open('responses/reponse_sc.json','w') as f:\n",
    "            json.dump(scq_response_json,f)\n",
    "else:\n",
    "    assert LOAD_SC_RESPONSE == True\n",
    "    with open('responses/reponse_sc.json','r') as f:\n",
    "        scq_response_json = json.load(f)\n",
    "print(scq_response_json.keys())\n",
    "pprint.pprint(scq_response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Scenario Description', 'Number of Humans', 'Trajectories', 'Behaviors', 'Expected Robot Behavior', 'Principle Ranking', 'Reasoning'])\n",
      "{'Behaviors': {'Human 1': 'Employee moves swiftly through the intersection but '\n",
      "                          'acknowledges the presence of the robot by making '\n",
      "                          'brief eye contact.'},\n",
      " 'Expected Robot Behavior': 'The robot waits at the intersection Aisle2 (2) '\n",
      "                            \"and says 'MAKE WAY' to the employee. Once the \"\n",
      "                            'employee has passed, the robot proceeds to Aisle7 '\n",
      "                            '(7) at a steady pace.',\n",
      " 'Number of Humans': 1,\n",
      " 'Principle Ranking': 'Safety (P1) > Politeness (P4) > Task Efficiency (P8) > '\n",
      "                      'Social Competency (P5) > Legibility (P3) > Comfort (P2) '\n",
      "                      '> Understanding Other Agents (P6) > Proactivity (P7)',\n",
      " 'Reasoning': \"This scenario tests the robot's ability to interact with \"\n",
      "              'employees in a busy warehouse environment where quick movements '\n",
      "              'and awareness are crucial. Prioritizing Safety and Politeness '\n",
      "              'ensures smooth navigation and minimal disruption to the '\n",
      "              \"employees' tasks.\",\n",
      " 'Scenario Description': 'The robot is navigating from Aisle1 (1) to Aisle7 '\n",
      "                         '(7) to perform an inventory check. An employee is '\n",
      "                         'walking from Aisle6 (6) to Hall (5) and crosses '\n",
      "                         'paths with the robot at the intersection Aisle2 (2).',\n",
      " 'Trajectories': {'Human 1': 'Aisle6 (6) -> Hall (5)',\n",
      "                  'Robot': 'Aisle1 (1) -> Hall (5) -> Aisle7 (7)'}}\n"
     ]
    }
   ],
   "source": [
    "print(scq_response_json.keys())\n",
    "pprint.pprint(scq_response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_HANDCRAFTED_SCENARIO:\n",
    "    scenario_desc = scenario_desc_hc\n",
    "    num_humans = num_humans\n",
    "    traj_desc = traj_desc_hc\n",
    "    behav_desc = behav_desc_hc\n",
    "else:\n",
    "    scenario_desc = scq_response_json['Scenario Description']\n",
    "    num_humans = scq_response_json['Number of Humans']\n",
    "    traj_desc = {}\n",
    "    for k,v in scq_response_json['Trajectories'].items():\n",
    "        traj_desc[k] = v.split(' -> ')\n",
    "    behav_desc = scq_response_json['Behaviors']\n",
    "\n",
    "# Assume the output is formatted correctly\n",
    "expected_robot_behav_desc = scq_response_json['Expected Robot Behavior'] \n",
    "pranking_desc = scq_response_json['Principle Ranking'] \n",
    "reasoning_desc = scq_response_json['Reasoning'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarse_locations:[7, 3, 6, 5, 2]\n"
     ]
    }
   ],
   "source": [
    "coarse_locations = []\n",
    "for k,v in traj_desc.items():\n",
    "    for loc in v:\n",
    "        match = re.search(r\"\\((\\d+)\\)\", loc)\n",
    "        if match:\n",
    "            l = int(match.group(1))\n",
    "            if l not in coarse_locations:\n",
    "                coarse_locations.append(l)\n",
    "print(f'coarse_locations:{coarse_locations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_loc_imgs = []\n",
    "[coarse_loc_imgs.append(os.path.join('locations',LOCATION,f'{img}.png')) for img in coarse_locations]\n",
    "#for loc in coarse_locations:\n",
    "#    coarse_loc_imgs[loc] = utils.encode_image(os.path.join('locations',location,f'{loc}.png'))\n",
    "#coarse_loc_imgs['overhead'] = utils.encode_image(os.path.join('locations',location,'overhead_annotated.png'))\n",
    "#coarse_loc_imgs = \n",
    "coarse_loc_imgs.append(os.path.join('locations',LOCATION,'overhead_annotated.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Locations Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flocationQ = FLocationQuery(\n",
    "    loc_imgs = coarse_loc_imgs\n",
    ")\n",
    "\n",
    "flq_full_prompt = flocationQ.get_full_prompt(\n",
    "    scene_graph = str(scene_graph),\n",
    "    num_humans = num_humans,\n",
    "    traj_desc = traj_desc,\n",
    "    sc_desc = scenario_desc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': 'You are a helpful assistant designed to output JSON',\n",
      " 'user': [{'content': 'locations/Warehouse/7.png',\n",
      "           'detail': 'high',\n",
      "           'type': 'img'},\n",
      "          {'content': 'locations/Warehouse/3.png',\n",
      "           'detail': 'high',\n",
      "           'type': 'img'},\n",
      "          {'content': 'locations/Warehouse/6.png',\n",
      "           'detail': 'high',\n",
      "           'type': 'img'},\n",
      "          {'content': 'locations/Warehouse/5.png',\n",
      "           'detail': 'high',\n",
      "           'type': 'img'},\n",
      "          {'content': 'locations/Warehouse/2.png',\n",
      "           'detail': 'high',\n",
      "           'type': 'img'},\n",
      "          {'content': 'locations/Warehouse/overhead_annotated.png',\n",
      "           'detail': 'high',\n",
      "           'type': 'img'},\n",
      "          {'content': '\\n'\n",
      "                      'A scenario for humans and robot is being created in a '\n",
      "                      'social-navigation simulator. The location where the '\n",
      "                      'scenario takes place has the following scene graph, '\n",
      "                      'where the nodes specify rough locations: \\n'\n",
      "                      \"[Scene Graph]:{'nodes': [{'type': 'narrow passage', \"\n",
      "                      \"'id': 'Aisle1 (1)'}, {'type': 'intersection', 'id': \"\n",
      "                      \"'Aisle2 (2)'}, {'type': 'open space', 'id': 'Aisle3 \"\n",
      "                      \"(3)'}, {'type': 'wide passage', 'id': 'Aisle (4)'}, \"\n",
      "                      \"{'type': 'open space', 'id': 'Hall (5)'}, {'type': \"\n",
      "                      \"'narrow passage', 'id': 'Aisle6 (6)'}, {'type': 'narrow \"\n",
      "                      \"passage', 'id': 'Aisle7 (7)'}], 'links': ['Aisle1 \"\n",
      "                      \"(1)<->Hall (5)', 'Aisle2 (2)<->Aisle3 (3)', 'Aisle2 \"\n",
      "                      \"(2)<->Hall (5)', 'Aisle3 (3)<->Hall (5)', 'Aisle \"\n",
      "                      \"(4)<->Hall (5)', 'Hall (5)<->Aisle6 (6)', 'Hall \"\n",
      "                      \"(5)<->Aisle7 (7)']}\\n\"\n",
      "                      'The Scenario is described below, with the number of '\n",
      "                      'humans involved and their trajectories defined using '\n",
      "                      'the scene graph nodes.\\n'\n",
      "                      '[Scenario Description]: The robot is at Aisle7 (7) and '\n",
      "                      'has to go to Aisle3 (3). At the same time, a human is '\n",
      "                      'moving from Aisle6 (6) to Aisle3 (3) through Hall (5) '\n",
      "                      'and another human is moving from Aisle6 (6) to Aisle2 '\n",
      "                      '(2) through Hall (5).\\n'\n",
      "                      '[Number of Humans]: 2\\n'\n",
      "                      '\\n'\n",
      "                      '[Human 1 Trajectory]: Aisle6 (6) -> Hall (5) -> Aisle3 '\n",
      "                      '(3)\\n'\n",
      "                      '            \\n'\n",
      "                      '[Human 2 Trajectory]: Aisle6 (6) -> Hall (5) -> Aisle2 '\n",
      "                      '(2)\\n'\n",
      "                      '            \\n'\n",
      "                      '[Robot Trajectory]: Aisle7 (7) -> Aisle3 (3)\\n'\n",
      "                      '\\n'\n",
      "                      '\\n'\n",
      "                      'The images show finer locations around the scene graph '\n",
      "                      'nodes.\\n'\n",
      "                      'Your job is to specify the start, goal and waypoints '\n",
      "                      'for the Robot and Humans such that the Described '\n",
      "                      'Scenario is guaranteed to happen. The humans and the '\n",
      "                      'robot start moving at the same time. Assume they move '\n",
      "                      'at the same sped.\\n'\n",
      "                      'When specifying finer locations, use the following '\n",
      "                      'format: (CoarseLocation-FineLocation). For example, 1A, '\n",
      "                      '2B etc.\\n'\n",
      "                      '\\n'\n",
      "                      'STRICTLY ADHERE TO THE FOLLOWING FORMAT FOR THE OUTPUT. '\n",
      "                      'Do not include any explanation.\\n'\n",
      "                      '\\n'\n",
      "                      \"{'Trajectory':\\n\"\n",
      "                      '    {\\n'\n",
      "                      '        \\'Robot\\': \"Start -> Goal\",\\n'\n",
      "                      '        \\'Human 1\\': \"Start ->  Waypoints -> Goal\",\\n'\n",
      "                      '        \\'Human 2\\': \"Start -> Waypoints -> Goal\",\\n'\n",
      "                      '        ...\\n'\n",
      "                      '    }\\n'\n",
      "                      '}',\n",
      "           'type': 'text'}]}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(flq_full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Fine Locations from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Trajectory'])\n",
      "{'Trajectory': {'Human 1': 'Aisle6-6A -> Hall-5B -> Aisle3-3B',\n",
      "                'Human 2': 'Aisle6-6B -> Hall-5A -> Aisle2-2A',\n",
      "                'Robot': 'Aisle7-7A -> Aisle3-3A'}}\n"
     ]
    }
   ],
   "source": [
    "if QUERY_LOC:\n",
    "    payload = model.get_payload(content = flq_full_prompt)\n",
    "    response = model.get_response(messages = payload,format = 'json_object')\n",
    "    flq_response_json = json.loads(response)\n",
    "    if SAVE_LOC_RESPONSE:\n",
    "        with open('responses/reponse_loc.json','w') as f:\n",
    "            json.dump(flq_response_json,f)\n",
    "else:\n",
    "    assert LOAD_LOC_RESPONSE == True\n",
    "    with open('responses/reponse_loc.json','r') as f:\n",
    "        flq_response_json = json.load(f)\n",
    "print(flq_response_json.keys())\n",
    "pprint.pprint(flq_response_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Robot': ['7A', '3A'], 'Human 1': ['6A', '5B', '3B'], 'Human 2': ['6B', '5A', '2A']}\n"
     ]
    }
   ],
   "source": [
    "trajectories = {}\n",
    "for k,v in flq_response_json['Trajectory'].items():\n",
    "    trajectories[k] = []\n",
    "    for loc in v.split(' -> '):\n",
    "        trajectories[k].append(loc.split('-')[1])\n",
    "print(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get world location of the trajectories\n",
    "with open(os.path.join('locations',LOCATION,'annotated_world_coords.pkl'),'rb') as f:\n",
    "    flwc = pkl.load(f)\n",
    "# with open(os.path.join('locations',LOCATION,'fine_loc_world_coords.pkl'),'rb') as f:\n",
    "#     flwc = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1A': (6.200000000000001, 8.85),\n",
       " '1B': (2.3000000000000007, 8.8),\n",
       " '1C': (3.6000000000000014, 8.85),\n",
       " '1D': (5.300000000000001, 8.85),\n",
       " '2A': (5.050000000000001, 4.699999999999999),\n",
       " '2B': (6.25, 4.699999999999999),\n",
       " '2C': (4.200000000000001, 5.1),\n",
       " '2D': (4.200000000000001, 6.2),\n",
       " '2E': (3.5500000000000007, 4.6499999999999995),\n",
       " '2F': (2.450000000000001, 4.75),\n",
       " '2G': (4.200000000000001, 3.9499999999999993),\n",
       " '2H': (4.15, 2.9499999999999993),\n",
       " '3A': (2.25, 2.4000000000000004),\n",
       " '3B': (2.450000000000001, 1.549999999999999),\n",
       " '3C': (6.15, 2.3499999999999996),\n",
       " '3D': (6.25, 1.299999999999999),\n",
       " '3E': (4.550000000000001, 1.1500000000000004),\n",
       " '3F': (4.5, 2.55),\n",
       " '4A': (3.450000000000001, 13.9),\n",
       " '4B': (1.5999999999999996, 13.85),\n",
       " '4C': (6.25, 14.0),\n",
       " '4D': (1.5, 12.8),\n",
       " '4E': (1.4000000000000004, 12.1),\n",
       " '5A': (-0.6499999999999995, 8.65),\n",
       " '5B': (1.5, 8.9),\n",
       " '5C': (-0.2999999999999998, 7.05),\n",
       " '5D': (0.5, 6.1),\n",
       " '5E': (1.5, 7.1),\n",
       " '5F': (-1.75, 7.9),\n",
       " '6A': (-0.25, 3.5),\n",
       " '6B': (-2.8, 3.5),\n",
       " '6C': (-0.34999999999999964, 2.299999999999999),\n",
       " '6D': (-0.25, 5.25),\n",
       " '6E': (-3.3499999999999996, 5.199999999999999),\n",
       " '6F': (-3.4499999999999997, 1.9499999999999993),\n",
       " '7A': (-0.6499999999999995, 8.15),\n",
       " '7B': (-0.6999999999999993, 8.95),\n",
       " '7C': (-0.39999999999999947, 7.449999999999999),\n",
       " '7D': (-3.05, 7.25),\n",
       " '7E': (-3.0, 7.75),\n",
       " '7F': (-2.95, 8.65),\n",
       " '7G': (-2.8499999999999996, 9.4),\n",
       " '8A': (-9.5, 7.15),\n",
       " '8B': (-6.35, 7.15),\n",
       " '8C': (-7.6, 6.3),\n",
       " '8D': (-9.5, 5.3999999999999995),\n",
       " '8E': (-6.05, 5.1499999999999995),\n",
       " '8F': (-9.25, 3.8499999999999996),\n",
       " '8G': (-6.15, 3.8999999999999995),\n",
       " '8H': (-7.4, 3.6999999999999993),\n",
       " '9A': (-8.9, 13.25),\n",
       " '9B': (-9.15, 12.0),\n",
       " '9C': (-7.6, 11.9),\n",
       " '9D': (-7.25, 13.4),\n",
       " '9E': (-5.6, 13.4),\n",
       " '9F': (-5.7, 11.8),\n",
       " '9G': (-6.5, 10.9),\n",
       " '10A': (-9.55, 2.75),\n",
       " '10B': (-6.35, 2.75),\n",
       " '10C': (-7.25, 2.9499999999999993),\n",
       " '10D': (-5.95, 1.5999999999999996),\n",
       " '10E': (-8.05, 0.75),\n",
       " '10F': (-9.35, 1.5),\n",
       " '10G': (-6.0, 0.29999999999999893),\n",
       " '10H': (-9.1, -0.20000000000000107)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flwc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "annotated_positions = {}\n",
    "starti = 1\n",
    "for k,v in flwc.items():\n",
    "    startc = 65    \n",
    "    for l in v:\n",
    "        annotated_positions[str(starti)+chr(startc)] = (l[0],-l[1])\n",
    "        startc+=1\n",
    "    starti+=1\n",
    "print(annotated_positions)\n",
    "with open(os.path.join('locations',LOCATION,'annotated_world_coords.pkl'),'wb') as f:\n",
    "    pkl.dump(annotated_positions,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Robot': [(-0.6499999999999995, 8.15), (2.25, 2.4000000000000004)], 'Human 1': [(-0.25, 3.5), (1.5, 8.9), (2.450000000000001, 1.549999999999999)], 'Human 2': [(-2.8, 3.5), (-0.6499999999999995, 8.65), (5.050000000000001, 4.699999999999999)]}\n"
     ]
    }
   ],
   "source": [
    "trajectories_world_coords = {}\n",
    "for k,v in trajectories.items():\n",
    "    trajectories_world_coords[k] = []\n",
    "    for loc in v:\n",
    "        trajectories_world_coords[k].append(flwc[loc])\n",
    "print(trajectories_world_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add fine locations to sim yaml files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents_yaml = {'hunav_loader': {'ros__parameters': {'map': LOCATION,\n",
    "   'publish_people': True,\n",
    "   'agents': []}}}\n",
    "blank_human = {'id': None,\n",
    "    'skin': 0,\n",
    "    'behavior': 0,\n",
    "    'group_id': -1,\n",
    "    'max_vel': 1.5,\n",
    "    'radius': 0.4,\n",
    "    'init_pose': {'x': None, 'y': None, 'z': 1.25, 'h': 0.0},\n",
    "    'goal_radius': 0.3,\n",
    "    'cyclic_goals': False,\n",
    "    'goals': [],\n",
    "    }\n",
    "agents = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_humans):\n",
    "    agents_yaml['hunav_loader']['ros__parameters']['agents'].append(f'agent{i}')\n",
    "    agents[f'agent{i}'] = copy.deepcopy(blank_human)\n",
    "    agents[f'agent{i}']['id'] = i\n",
    "    agents[f'agent{i}']['behavior'] = 7+i\n",
    "    for j,g in enumerate(trajectories_world_coords[f'Human {i+1}']):\n",
    "        if j == 0:\n",
    "            agents[f'agent{i}']['init_pose'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'z':1.25,\n",
    "                'h':0.0,\n",
    "            } \n",
    "        else:\n",
    "            agents[f'agent{i}']['goals'].append(f'g{j}')\n",
    "            agents[f'agent{i}'][f'g{j}'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'h':1.25\n",
    "            }\n",
    "agents_yaml['hunav_loader']['ros__parameters'].update(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HUNAV_SIM_AGENTS_FILE,'w') as f:\n",
    "    yaml.dump(agents_yaml,f)\n",
    "\n",
    "with open(os.path.join(HUNAV_GAZEBO_WRAPPER_DIR,'config','robot.yaml'),'w') as f:\n",
    "    yaml.dump({\n",
    "    'x_pose': trajectories_world_coords['Robot'][0][0],\n",
    "    'y_pose': trajectories_world_coords['Robot'][0][1]\n",
    "},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM for BT for humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Custom Action 1': 'Design an Action node called SlowNav(agent_id, t) with '\n",
      "                    'the following logic: The agent navigates through the '\n",
      "                    'environment at a slower pace compared to regular '\n",
      "                    'navigation, taking into account reduced time steps for '\n",
      "                    'movement.',\n",
      " 'Reasoning': \"The human's behavior should check for robot visibility within a \"\n",
      "              '1m range. If the robot is visible, they will perform the '\n",
      "              'SurprisedNav action for 3 seconds. Otherwise, a slow navigation '\n",
      "              'behavior is needed; since this behavior is not present in the '\n",
      "              'provided nodes, it will have to be a custom action node named '\n",
      "              'SlowNav.',\n",
      " 'Tree': '<root main_tree_to_execute = \"StartledSlowNavTree\">\\n'\n",
      "         '\\n'\n",
      "         '    <TreeNodesModel>\\n'\n",
      "         '        <Action ID=\"SurprisedNav\">\\n'\n",
      "         '            <input_port name=\"agent_id\" type=\"int\">identifier of the '\n",
      "         'agent</input_port>\\n'\n",
      "         '            <input_port name=\"time_step\" type=\"double\">time step in '\n",
      "         'seconds to compute movement</input_port>\\n'\n",
      "         '        </Action>\\n'\n",
      "         '        <Action ID=\"SlowNav\">\\n'\n",
      "         '            <input_port name=\"agent_id\" type=\"int\">identifier of the '\n",
      "         'agent</input_port>\\n'\n",
      "         '            <input_port name=\"time_step\" type=\"double\">reduced time '\n",
      "         'step in seconds for slower movement</input_port>\\n'\n",
      "         '        </Action>\\n'\n",
      "         '        <Condition ID=\"IsRobotVisible\">\\n'\n",
      "         '            <input_port name=\"agent_id\" type=\"int\">identifier of the '\n",
      "         'agent</input_port>\\n'\n",
      "         '            <input_port name=\"distance\" '\n",
      "         'type=\"double\">1.0</input_port>\\n'\n",
      "         '        </Condition>\\n'\n",
      "         '        <Condition ID=\"TimeExpiredCondition\">\\n'\n",
      "         '            <input_port name=\"seconds\" type=\"double\">duration of the '\n",
      "         'timer in seconds</input_port>\\n'\n",
      "         '            <input_port name=\"ts\" type=\"double\">time step to be '\n",
      "         'accumulated</input_port>\\n'\n",
      "         '            <input_port name=\"only_once\" type=\"bool\">reset timer '\n",
      "         'flag</input_port>\\n'\n",
      "         '        </Condition>\\n'\n",
      "         '    </TreeNodesModel>\\n'\n",
      "         '\\n'\n",
      "         '    <BehaviorTree ID=\"StartledSlowNavTree\">\\n'\n",
      "         '        <Fallback name=\"VisuallyImpairedFallback\">\\n'\n",
      "         '            <Sequence name=\"StartledSequence\">\\n'\n",
      "         '                <IsRobotVisible agent_id=\"{id}\" distance=\"1.0\" />\\n'\n",
      "         '                <Sequence>\\n'\n",
      "         '                    <Inverter>\\n'\n",
      "         '                        <TimeExpiredCondition seconds=\"3.0\" '\n",
      "         'ts=\"{dt}\" only_once=\"True\" />\\n'\n",
      "         '                    </Inverter>\\n'\n",
      "         '                    <SurprisedNav agent_id=\"{id}\" time_step=\"{dt}\" '\n",
      "         '/>\\n'\n",
      "         '                </Sequence>\\n'\n",
      "         '            </Sequence>\\n'\n",
      "         '            <SlowNav agent_id=\"{id}\" time_step=\"{dt}\" />\\n'\n",
      "         '        </Fallback>\\n'\n",
      "         '    </BehaviorTree>\\n'\n",
      "         '\\n'\n",
      "         '</root>'}\n",
      "----\n",
      "Wrote BT to LLMBT_0.xml\n",
      "{'Reasoning': 'To create this behavior, we can use the CuriousNav action when '\n",
      "              'the robot is visible to represent the human walking slowly '\n",
      "              'towards the robot. To disengage after 5 seconds, we use the '\n",
      "              \"TimeExpiredCondition. If the robot isn't visible or the timer \"\n",
      "              'expires, the human proceeds with the regular navigation using '\n",
      "              'RegularNav.',\n",
      " 'Tree': '<root main_tree_to_execute = \"CuriousNavTree\">\\n'\n",
      "         '\\n'\n",
      "         '    <TreeNodesModel>\\n'\n",
      "         '        <Action ID=\"CuriousNav\">\\n'\n",
      "         '            <input_port name=\"agent_id\" type=\"int\">identifier of the '\n",
      "         'agent</input_port>\\n'\n",
      "         '            <input_port name=\"time_step\" type=\"double\">time step in '\n",
      "         'seconds to compute movement</input_port>\\n'\n",
      "         '        </Action>\\n'\n",
      "         '        <Condition ID=\"IsRobotVisible\">\\n'\n",
      "         '            <input_port name=\"agent_id\" type=\"int\">identifier of the '\n",
      "         'agent</input_port>\\n'\n",
      "         '        </Condition>\\n'\n",
      "         '        <Condition ID=\"TimeExpiredCondition\">\\n'\n",
      "         '            <input_port name=\"seconds\" type=\"double\">duration of the '\n",
      "         'timer in seconds</input_port>\\n'\n",
      "         '            <input_port name=\"ts\" type=\"double\">time step to be '\n",
      "         'accumulated</input_port>\\n'\n",
      "         '            <input_port name=\"only_once\" type=\"bool\">boolean to '\n",
      "         'indicate if the timer must be reset at the end or not</input_port>\\n'\n",
      "         '        </Condition>\\n'\n",
      "         '    </TreeNodesModel>\\n'\n",
      "         '\\n'\n",
      "         '    <include path=\"BTRegularNav.xml\"/>\\n'\n",
      "         '\\n'\n",
      "         '    <BehaviorTree ID=\"CuriousNavTree\">\\n'\n",
      "         '        <Fallback name=\"CuriousFallback\">\\n'\n",
      "         '            <Sequence name=\"CurNav\">\\n'\n",
      "         '                <IsRobotVisible agent_id=\"{id}\" distance=\"10.0\" />\\n'\n",
      "         '                <TimeExpiredCondition seconds=\"5.0\" ts=\"{dt}\" '\n",
      "         'only_once=\"True\" />\\n'\n",
      "         '                <CuriousNav agent_id=\"{id}\" time_step=\"{dt}\" />\\n'\n",
      "         '            </Sequence>\\n'\n",
      "         '            <Sequence name=\"RegNav\">\\n'\n",
      "         '                <SetBlackboard output_key=\"agentid\" value=\"{id}\" />\\n'\n",
      "         '                <SetBlackboard output_key=\"timestep\" value=\"{dt}\" '\n",
      "         '/>\\n'\n",
      "         '                <SubTree ID=\"RegularNavTree\" id=\"agentid\" '\n",
      "         'dt=\"timestep\" />\\n'\n",
      "         '            </Sequence>\\n'\n",
      "         '        </Fallback>\\n'\n",
      "         '    </BehaviorTree>\\n'\n",
      "         '</root>'}\n",
      "----\n",
      "Wrote BT to LLMBT_1.xml\n"
     ]
    }
   ],
   "source": [
    "custom_node_requests = []\n",
    "for i in range(num_humans):\n",
    "    btq = BTQuery()\n",
    "    if QUERY_BT:\n",
    "        print(behav_desc[f'Human {i+1}'])\n",
    "        btq_full_prompt = btq.get_full_prompt(behavior = behav_desc[f'Human {i+1}'])\n",
    "        payload = model.get_payload(content = btq_full_prompt)\n",
    "        response = model.get_response(messages = payload,format = 'json_object')\n",
    "        btq_response_json = json.loads(response)\n",
    "        if SAVE_BT_RESPONSE:\n",
    "            with open(f'responses/reponse_bt_{i+1}.json','w') as f:\n",
    "                json.dump(btq_response_json,f)\n",
    "    else:\n",
    "        assert LOAD_BT_RESPONSE == True\n",
    "        with open(f'responses/reponse_bt_{i+1}.json','r') as f:\n",
    "            btq_response_json = json.load(f)\n",
    "        #print(btq_response_json.keys())\n",
    "    pprint.pprint(btq_response_json)\n",
    "    print('----')\n",
    "    bt_xml = btq_response_json['Tree']\n",
    "    for k,v in btq_response_json.items():\n",
    "        if 'custom' in str.lower(k):\n",
    "            custom_node_requests.append(v)\n",
    "    with open(os.path.join(HUNAV_SIM_BT_FOLDER,f'LLMBT_{i}.xml'),'w') as f:\n",
    "        f.write(bt_xml)\n",
    "    print(f\"Wrote BT to LLMBT_{i}.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query LLM for custom Nodes and Auxillary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Design an Action node called SlowNav(agent_id, t) with the following logic: The agent navigates through the environment at a slower pace compared to regular navigation, taking into account reduced time steps for movement.']\n"
     ]
    }
   ],
   "source": [
    "print(custom_node_requests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(custom_node_requests)):\n",
    "    if QUERY_AUX:\n",
    "        ctnq = NodeQuery()\n",
    "        ctnq_full_prompt = ctnq.get_full_prompt(description = custom_node_requests[i])\n",
    "        payload = model.get_payload(content = ctnq_full_prompt)\n",
    "        response = model.get_response(messages = payload,format = 'json_object')\n",
    "        ctnq_response_json = json.loads(response)\n",
    "        if SAVE_AUX_RESPONSE:\n",
    "            with open('responses/reponse_aux.json','w') as f:\n",
    "                json.dump(ctnq_response_json,f)\n",
    "    else:\n",
    "        assert LOAD_AUX_RESPONSE == True\n",
    "        with open('responses/reponse_aux.json','r') as f:\n",
    "            ctnq_response_json = json.load(f)\n",
    "    with open(os.path.join('templates','extended_bt_functions.cpp'),'r') as f:\n",
    "        btf_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_functions.hpp'),'r') as f:\n",
    "        btf_hpp = f.read()\n",
    "\n",
    "    #register node in bt_node.cpp\n",
    "    with open(os.path.join('templates','extended_agent_manager.cpp'),'r') as f:\n",
    "        agm_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_agent_manager.hpp'),'r') as f:\n",
    "        agm_hpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_node.cpp'),'r') as f:\n",
    "        btn_cpp = f.read()\n",
    "\n",
    "    with open(os.path.join('templates','extended_bt_node.hpp'),'r') as f:\n",
    "        btn_hpp = f.read()\n",
    "    \n",
    "    #Write functions to extended_bt_functions.cpp file\n",
    "    btf = ctnq_response_json['NODE_DEFINITION']\n",
    "    btf = btf.replace('BT::NodeStatus BTfunctions::','BT::NodeStatus BTfunctionsExt::')\n",
    "    btf_name = ctnq_response_json['NODE_NAME']\n",
    "    btf_type = ctnq_response_json['NODE_TYPE'].lower().capitalize()\n",
    "    btfn_name = btf_name[0].lower() + btf_name[1:]\n",
    "    btf_header = ctnq_response_json['NODE_HEADER']\n",
    "\n",
    "    btf_cpp = btf_cpp.replace('//<NEW FUNCTION>','//<NEW FUNCTION> \\n' + btf)\n",
    "    btf_hpp = btf_hpp.replace('//<NEW PUBLIC FUNCTION>','//<NEW PUBLIC FUNCTION> \\n' + btf_header)\n",
    "    \n",
    "    #register BT nodes in extended_bt_node.cpp\n",
    "    #   3 ports are available for each BT node:\n",
    "    #       simple_port: agent_id\n",
    "    #       visibleports: agent_id + distance\n",
    "    #       portsNav: agent_id + timestep\n",
    "    \n",
    "    node_register = f\"\"\"factory_.registerSimple{btf_type}(\"{ctnq_response_json['NODE_NAME']}\",std::bind(&BTfunctionsExt::{btfn_name},&btfunc_, _1),PORT);\"\"\"\n",
    "    if ctnq_response_json['PORTS_USED'] == ['agent_id','distance']:\n",
    "        node_register = node_register.replace('PORT','visibleports')\n",
    "    elif ctnq_response_json['PORTS_USED'] == ['agent_id','time_step']:\n",
    "        node_register = node_register.replace('PORT','portsNav')\n",
    "    elif ctnq_response_json['PORTS_USED'] == ['agent_id']:\n",
    "        node_register = node_register.replace('PORT','simple_port')\n",
    "    else:\n",
    "        node_register = node_register.replace(',PORT','')\n",
    "\n",
    "    btn_cpp = btn_cpp.replace('//<NEW NODE REGISTER>','//<NEW NODE REGISTER> \\n' + node_register)   \n",
    "\n",
    "    #add aux functions\n",
    "    for j,agmf in enumerate(ctnq_response_json['AUX_FUNCTIONS']):\n",
    "        agmf = agmf.replace('void AgentManager::','void AgentManagerExt::')\n",
    "        agm_cpp = agm_cpp.replace('//<NEW FUNCTION>','//<NEW FUNCTION> \\n' + agmf)\n",
    "        agm_hpp = agm_hpp.replace('//<NEW PUBLIC FUNCTION>','//<NEW PUBLIC FUNCTION> \\n' + ctnq_response_json['AUX_FUNCTION_HEADERS'][j])\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_bt_functions.cpp'),'w') as f:\n",
    "    f.writelines(btf_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_bt_functions.hpp'),'w') as f:\n",
    "    f.writelines(btf_hpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_bt_node.cpp'),'w') as f:\n",
    "    f.writelines(btn_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_bt_node.hpp'),'w') as f:\n",
    "    f.writelines(btn_hpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_CPP_FOLDER,'extended_agent_manager.cpp'),'w') as f:\n",
    "    f.writelines(agm_cpp)\n",
    "\n",
    "with open(os.path.join(HUNAV_SIM_HPP_FOLDER,'extended_agent_manager.hpp'),'w') as f:\n",
    "    f.writelines(agm_hpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Successful\n"
     ]
    }
   ],
   "source": [
    "s = subprocess.getstatusoutput(f' cd ~/catkin_ws && colcon build')\n",
    "if s[0] == 0:\n",
    "    print('Build Successful')\n",
    "else:\n",
    "    print('Build Failed')\n",
    "    print(s[1]) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
