{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Scene graph and image of map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "from PIL import Image as PILImage\n",
    "from Prompts import *\n",
    "import pickle as pkl\n",
    "import yaml\n",
    "from utils.config import *\n",
    "from utils.models import *\n",
    "import copy\n",
    "import subprocess\n",
    "\n",
    "model = GPTModel(config = dict(\n",
    "    MODEL_NAME = MODEL\n",
    "))\n",
    "\n",
    "with open(os.path.join('locations',LOCATION,'scene_graph.json'),'r') as f:\n",
    "    scene_graph = json.load(f)\n",
    "\n",
    "scgraph = utils.SceneGraph(scene_graph)\n",
    "encoded_img = utils.encode_image(os.path.join('locations',LOCATION,'scene_graph.png'))\n",
    "node_types = []\n",
    "for node in scgraph.get_parent_nodes():\n",
    "    if scgraph.graph.nodes[node]['type'] not in node_types:\n",
    "        node_types.append(scgraph.graph.nodes[node]['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Querying LLM for Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scQ = ScenarioQuery()\n",
    "scQ_full_prompt = scQ.get_full_prompt(\n",
    "    context=CONTEXT,\n",
    "    task=TASK,\n",
    "    rough_scenario='',\n",
    "    location = LOCATION_DESC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: [{'type': 'text', 'text': 'Provide outputs strictly in JSON format'}]\n",
      "\u001b[0m\n",
      "\u001b[32muser: [{'type': 'text', 'text': '\\n                                      \\nSocial Navigation is a complex skill for a robot to accomplish and the appropriateness of the behavior of a robot is highly dependent on the task and the social context. \\nThus a robot’s social navigation capabilities must be thoroughly tested, and this is done by evaluating the robot’s behavior in a number of scenarios in a variety of contexts.\\nYou are a scenario designer. Your task is to generate scenarios to test the social navigation capabilities of a robot.\\nA Social Navigation [Scenario] is defined by:\\n    1. Scenario Description: very detailed description of the scenario. WHAT happens in the scenario and WHERE the scenario takes place. WHERE the robot and humans are located.\\n    2. Human Behavior:  how human interacts with the robot when it is visible, for e.g. Human 1 is scared of the robot and asks it to stop, Human 2 doesn\\'t notice the robot at all etc.\\nYour output description will be later used by an expert Behaviour tree designer to generate a Behavior Tree for each human in the scene. \\n\\nThe behavior tree designer is not allowed to modify the scenario and can only create behavior that can be generated using the following Actions and Conditions:\\n - Conditions\\n        - Check the visibility of the robot\\n        - Check if the human has reached their goal\\n        - Check if robot is saying any particular phrase\\n        - Check if the robot is currently moving\\n        - Check if the robot is blocking the human\\'s path\\n    \\n    - Actions:\\n        - Make the human perform a gesture.\\n        - Make the human perform normal navigation to reach its goal and treat the robot as a normal obstalce. This is regular behavior for humans.\\n        - Make the human look in the direction of the robot\\n        - Make the human follow the robot\\n        - Make the human scared of the robot and avoid it.\\n        - Make the human give way to the robot\\n        - Make the human move quickly towards the front of the robot and block the robot.\\n        \\n        NOTE: AT ANY GIVEN POINT OF TIME, THE HUMAN CAN ONlY PEFORM ANY ONE OF THE ABOVE ACTIONS.\\n\\nThe humans are only capable of performing the actions mentioned above.\\nUser will provide a [Social context], a [Task] that the robot needs to do, a description of the location and optionally a [Rough Scenario]. \\nYour generated scenario will be programmatically simulated through a pipeline into a scenario in the Gazebo physics simulator.\\nRules:\\n- The humans can say \"WAIT\", \"PROCEED\", \"EXCUSE ME\" to the robot  to aid in navigation. The robot can say \"WAIT\", \"PROCEED\", \"EXCUSE ME\", \"ACKNOWLEDGED\" to the humans to aid in navigation. \\n- When the user provides a Rough Scenario, ensure your final scenario is strictly aligned to the rough scenario\\n- The humans in the simulator are SIMPLIFIED OBJECTS that only can move in 2D, send and receive simple phrases, detect and simulate looking at the robot, group together with other humans, navigate to a predefined goal and change their trajectory conditioned on the robot’s position and velocity.\\n- When using groups in the scenario, add all group members to the humans in the scenario. Having only 1 human with \\'INTERACTING WITH GROUP\\' task is incorrect.\\n\\nDesign a scenario relevant to the following specifications:\\n\\n[Social context]: Robot is a home assistant in a Singaporean old-age home and performs daily helpful duties for the residents\\n[Robot Task]: Deliver coffee\\n[Rough Scenario]: None\\n[Location]:   The home has a Kitchen, a Bedroom and a Living Room. A doorway connects the Kitchen to the Living Room and a Passageway connects the Living Room and the Doorway.\\n\\nYOU ADHERE TO THE FOLLOWING JSON FORMAT STRICTLY. \\n{\\n\\'Scenario Description\\': <very detailed description of the scenario >,\\n\\'Number of Humans\\': <Number of humans that are involved in the scenario>,\\n‘Human behavior\\': {\\n‘Human 1’: <Describe the behavior of Human 1>,\\n‘Human 2’: <Describe the behavior of Human 2>,\\n},\\n\\'Expected Robot Behavior\\': <Describe the behavior expected from the robot>\\n}\\n'}]\n",
      "\u001b[0m\n",
      "\u001b[34massistant: [{'type': 'text', 'text': '\\n            {\\'Scenario Description\\': \"The robot is trying to deliver coffee from the Kitchen to the Living Room and encounters one of the elderly residents entering the Kitchen from the Living Room through the  Doorway.\",\\n            \\'Number of Humans\\': 1,\\n            \\'Human Behavior\\':{\\n            \\'Human 1\\': Human 1 is going from going to kitchen from the living room. If the robot is very close-by, Human asks the robot to stop and waits (for a maximum of 5s) for the robot to stop, then continues navigating. Ignores the robot if it asks the human to wait.\\n            },\\n            \\'Expected Robot Behavior\\': \"The robot says \"I AM HERE\" to the resident. It waits for the resident to be well clear of the Doorway before going through the Doorway to the Living Room in a slow pace.\"\\n            }\\n            '}]\n",
      "\u001b[0m\n",
      "\u001b[32muser: [{'type': 'text', 'text': \"\\nDesign a scenario relevant to the following specifications:\\n\\n[Social Context]:Robot is an maintenance robot inside a warehouse.\\n[Task]:The task of the robot is to clean the warehouse.\\n[Location]:The Location is a Small Warehouse consists of various racks and open packaging areas connected by Passageways. The Passageways also create Intersections.\\n[Rough Scenario]:\\n\\nYOU ADHERE TO THE FOLLOWING JSON FORMAT STRICTLY. \\n{\\n'Scenario Description': <very detailed description of the scenario >,\\n'Number of Humans': <Number of humans that are involved in the scenario>,\\n‘Human behavior': {\\n‘Human 1’: <Describe the behavior of Human 1>,\\n‘Human 2’: <Describe the behavior of Human 2>,\\n},\\n'Expected Robot Behavior': <Describe the behavior expected from the robot>\\n}\\n\\n\"}]\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "utils.pretty_print_conversation(scQ_full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying LLM for scenario\n",
      "Recieved JSON Parseable output\n",
      "Saving scenario response\n",
      "================PROPOSED SCENARIO:===============\n",
      "{'scenariodescription': 'The robot is tasked with cleaning the warehouse. It starts in an open packaging area and moves toward one of the passageways to clean the floor. As it moves, it encounters two warehouse workers at different intersections: one worker is transporting items using a trolley toward the open packaging area, while the other worker is walking through the passageway, heading toward the racks.', 'numberofhumans': 2, 'humanbehavior': {'Human 1': \"Human 1 is transporting items using a trolley from one rack to the open packaging area. If Human 1 sees the robot in the passageway, they ask the robot to wait by saying 'WAIT' and then proceed to move the trolley past the robot before continuing on their path. Human 1 will then ignore the robot once past.\", 'Human 2': \"Human 2 is walking through the passageway toward the racks. If Human 2 sees the robot, they say 'EXCUSE ME' and wait for the robot to either stop or move aside. Once the passageway is clear, Human 2 resumes normal navigation to their goal.\"}, 'expectedrobotbehavior': \"The robot should wait when Human 1 says 'WAIT'. It resumes cleaning once Human 1 has moved the trolley past. When Human 2 says 'EXCUSE ME', the robot should either stop if it's moving or move aside if it's stationary. The robot should ensure the passageway is clear before resuming its cleaning task in the passageway.\"}\n",
      "USING GENERATED SCENARIO\n",
      "Querying LLM for scenario\n",
      "Recieved JSON Parseable output\n",
      "Saving scenario response\n",
      "================PROPOSED SCENARIO:===============\n",
      "{'scenariodescription': 'The robot is cleaning the Passageway B of the warehouse when it encounters two warehouse workers. Human 1 is moving from the open packaging area to rack section A and Human 2 is moving from rack section B to the open packaging area. The intersection in the Passageway B is a tight area where only one entity can safely pass at a time.', 'numberofhumans': 2, 'humanbehavior': {'Human 1': \"While navigating from the packaging area to rack section A, Human 1 notices the robot and says 'EXCUSE ME'. Human 1 then waits for the robot to acknowledge and give way before proceeding.\", 'Human 2': 'Human 2 is moving quickly from rack section B to the open packaging area and does not notice the robot initially. Upon seeing Human 1 interacting with the robot, Human 2 slows down and waits behind Human 1.'}, 'expectedrobotbehavior': \"The robot acknowledges Human 1 by saying 'ACKNOWLEDGED', stops its cleaning operation, and moves aside to give way to Human 1 and Human 2. Once both humans have passed through the intersection, the robot resumes its cleaning task.\"}\n",
      "USING GENERATED SCENARIO\n",
      "Querying LLM for scenario\n",
      "Recieved JSON Parseable output\n",
      "Saving scenario response\n",
      "================PROPOSED SCENARIO:===============\n",
      "{'scenariodescription': 'The robot is cleaning an open packaging area in the warehouse. Human 1 is organizing boxes on a shelf in the corner of the same area, while Human 2 is walking from one end of the packaging area to the other, through the intersection. Human 2 crosses paths with the robot multiple times as it cleans.', 'numberofhumans': 2, 'humanbehavior': {'Human 1': 'Human 1 occasionally looks towards the robot but largely ignores it and continues organizing boxes. If the robot comes too close, Human 1 gives way to the robot and then resumes organizing.', 'Human 2': \"Human 2 regularly crosses paths with the robot. When the robot is visible, Human 2 says 'EXCUSE ME' and continues normal navigation, treating the robot as an obstacle. If the robot blocks Human 2's path, Human 2 says 'WAIT' and waits for the robot to move before continuing.\"}, 'expectedrobotbehavior': \"The robot should navigate around the humans while cleaning. If Human 2 says 'EXCUSE ME', the robot should momentarily slow down and give way. If Human 2 says 'WAIT', the robot should stop immediately and say 'ACKNOWLEDGED' before moving out of Human 2's path.\"}\n",
      "USING GENERATED SCENARIO\n",
      "Querying LLM for scenario\n",
      "Recieved JSON Parseable output\n",
      "Saving scenario response\n",
      "================PROPOSED SCENARIO:===============\n",
      "{'scenariodescription': 'The robot is tasked with cleaning the warehouse and starts by moving from the central Passageway to the eastern end where there is an open packaging area. There are racks on either side of the passageway. As the robot moves along the passageway, it encounters a worker who is inspecting the racks and occasionally crossing the passageway to pick items from opposite shelves. The robot must navigate around the worker while continuing its cleaning task.', 'numberofhumans': 2, 'humanbehavior': {'Human 1': \"Human 1 is inspecting items on the racks along the passageway and occasionally crossing the passageway to pick items from the opposite shelves. When the robot is visible, Human 1 glances at the robot but continues with their task. If the robot is blocking the path, Human 1 waits briefly and says 'EXCUSE ME' before continuing to the other side.\", 'Human 2': 'Human 2 is at the open packaging area at the eastern end of the warehouse, packaging items. They do not notice the robot unless it enters their immediate vicinity. If the robot comes close, Human 2 steps aside to give way to the robot without any verbal communication.'}, 'expectedrobotbehavior': \"The robot should avoid blocking Human 1's path by stopping and saying 'ACKNOWLEDGED' when it hears 'EXCUSE ME', then move aside to let Human 1 pass. In the open packaging area, the robot should slow down if Human 2 is in its immediate vicinity and resume cleaning once Human 2 steps aside.\"}\n",
      "USING GENERATED SCENARIO\n",
      "Querying LLM for scenario\n",
      "Recieved JSON Parseable output\n",
      "Saving scenario response\n",
      "================PROPOSED SCENARIO:===============\n",
      "{'scenariodescription': 'The robot is cleaning the Passageways within the warehouse. It starts from an open packaging area and moves towards an intersection. At the intersection, two warehouse workers enter from different directions, both heading towards the open packaging area. The intersection is relatively narrow, making it challenging for the robot and the humans to pass simultaneously.', 'numberofhumans': 2, 'humanbehavior': {'Human 1': \"Human 1 enters the intersection from passageway A. If Human 1 sees the robot at the intersection and the robot is currently moving, Human 1 says 'WAIT' and waits for the robot to stop before proceeding. Otherwise, Human 1 continues navigating normally to the packaging area.\", 'Human 2': 'Human 2 enters the intersection from passageway B. Human 2 does not notice the robot and continues navigating normally towards the packaging area, treating the robot as a normal obstacle.'}, 'expectedrobotbehavior': \"The robot stops when it hears a human say 'WAIT' and says 'ACKNOWLEDGED'. It waits for both humans to clear the intersection before resuming its cleaning task and continuing to passageway.\"}\n",
      "USING GENERATED SCENARIO\n",
      "The robot is cleaning the Passageways within the warehouse. It starts from an open packaging area and moves towards an intersection. At the intersection, two warehouse workers enter from different directions, both heading towards the open packaging area. The intersection is relatively narrow, making it challenging for the robot and the humans to pass simultaneously.\n",
      "Number of Humans:2\n",
      "Behaviors:{'Human 1': \"Human 1 enters the intersection from passageway A. If Human 1 sees the robot at the intersection and the robot is currently moving, Human 1 says 'WAIT' and waits for the robot to stop before proceeding. Otherwise, Human 1 continues navigating normally to the packaging area.\", 'Human 2': 'Human 2 enters the intersection from passageway B. Human 2 does not notice the robot and continues navigating normally towards the packaging area, treating the robot as a normal obstacle.'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    if QUERY_SC:    \n",
    "        print(\"Querying LLM for scenario\")\n",
    "        scq_response_json = model.get_response(messages = scQ_full_prompt,format = \"json_object\",expected_keys=scQ.required_output_keys)\n",
    "        if SAVE_SC_RESPONSE:\n",
    "            print(\"Saving scenario response\")\n",
    "            with open(os.path.join(SAVE_DIR,f'response_sc_{i}.json'),'w') as f:\n",
    "                json.dump(scq_response_json,f)\n",
    "                \n",
    "        print(\"================PROPOSED SCENARIO:===============\")\n",
    "        print(scq_response_json)\n",
    "        # user_input = input(\"Continue with proposed scenario? ('no' to requery) (yes/no): \")\n",
    "        # if user_input.lower() == \"yes\" or user_input.lower() == \"y\":\n",
    "        #     print(\"Continuing...\")\n",
    "        #     break\n",
    "        \n",
    "        print('USING GENERATED SCENARIO')\n",
    "        scenario_desc = scq_response_json['scenariodescription']\n",
    "        num_humans = scq_response_json['numberofhumans']\n",
    "        behav_desc = scq_response_json['humanbehavior']\n",
    "\n",
    "    else:\n",
    "        if USE_HANDCRAFTED_SCENARIO:\n",
    "            print('USING HANDCRAFTED SCENARIO')\n",
    "            print('Scenario Description:')\n",
    "            scenario_desc = scenario_desc_hc\n",
    "            num_humans = num_humans_hc\n",
    "            behav_desc = behav_desc_hc\n",
    "        else:\n",
    "            print(\"Loading prior scenario response\")\n",
    "            with open(os.path.join(SAVE_DIR,'response_sc.json'),'r') as f:\n",
    "                scq_response_json = json.load(f)\n",
    "            \n",
    "            scq_response_json = {k.lower().replace('_','').replace(' ',''): v for k,v in scq_response_json.items()}\n",
    "            scenario_desc = scq_response_json['scenariodescription']\n",
    "            behav_desc =  {k.lower().replace('_','').replace(' ',''): v for k,v in scq_response_json['humanbehavior'].items()}\n",
    "            num_humans =  scq_response_json['numberofhumans']\n",
    "\n",
    "\n",
    "            \n",
    "print(scenario_desc)\n",
    "print(f'Number of Humans:{num_humans}')\n",
    "print(f'Behaviors:{behav_desc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Querying LLM for Human and Robot Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flq = FLocationQuery()\n",
    "\n",
    "flq_full_prompt = flq.get_full_prompt(\n",
    "    scene_graph = str(scene_graph),\n",
    "    node_types = ','.join(node_types),\n",
    "    encoded_img = encoded_img,\n",
    "    sc_desc = scenario_desc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.pretty_print_conversation(flq_full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_TRAJ:\n",
    "    print(\"Querying LLM for Trajectories\")\n",
    "    valid_trajectories = 0\n",
    "    all_trajectories_valid = False\n",
    "    payload = flq_full_prompt.copy()\n",
    "    while not all_trajectories_valid:\n",
    "        retries = -1\n",
    "        payload = flq_full_prompt.copy()\n",
    "        while retries <=3: #retry until trajectories are valid (max 3)\n",
    "            all_trajectories_valid = True\n",
    "            flq_response_json = model.get_response(messages = payload,format = 'json_object',expected_keys=flq.required_output_keys)\n",
    "            if not isinstance(flq_response_json,dict):\n",
    "                retries+=1\n",
    "                continue\n",
    "            trajectories = flq_response_json['trajectories']\n",
    "            groupids = flq_response_json['groupids']\n",
    "            traj_valid = True\n",
    "            #test connectivity\n",
    "            for k,v in trajectories.items():\n",
    "                traj_valid,errors = scgraph.isvalidtrajectory(v)\n",
    "                if not traj_valid: #requery LLM with error message\n",
    "                    print(\"Disconnected Trajectory Output, Retrying\")\n",
    "                    valid_trajectories=0\n",
    "                    all_trajectories_valid = False\n",
    "                    reply = f\"\"\"\n",
    "                            You made the following mistakes:\n",
    "                            <ERRORS>\n",
    "                            Retry and Return the answer in the same JSON format.\n",
    "                            \"\"\"\n",
    "                    error_string = \"\"\n",
    "                    for err in errors:\n",
    "                        error_string+=f\"There is no edge connecting {err[0]} and {err[1]}!\"\n",
    "                        \n",
    "                    reply = reply.replace('<ERRORS>',error_string)\n",
    "                    payload.append(\n",
    "                        {\n",
    "                            \"role\":\"assistant\",\n",
    "                            \"content\":[\n",
    "                                {\n",
    "                                    \"type\":\"text\",\n",
    "                                    \"text\": str(flq_response_json)\n",
    "                                }\n",
    "                            ]\n",
    "                        }    \n",
    "                        )\n",
    "                    payload.append({\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\":\"text\",\n",
    "                                    \"text\": reply\n",
    "                                }\n",
    "                                    ]\n",
    "                                }\n",
    "                                )\n",
    "                    \n",
    "                    retries+=1\n",
    "                    break\n",
    "                else:\n",
    "                    valid_trajectories+=1\n",
    "        \n",
    "            if all_trajectories_valid:\n",
    "                break\n",
    "    groupids =  {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json['groupids'].items()}\n",
    "    trajectories =  {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json['trajectories'].items()}\n",
    "\n",
    "    if SAVE_TRAJ_RESPONSE:\n",
    "        print(\"Saving trajectory response\")\n",
    "        #with open('responses/reponse_traj.json','w') as f:\n",
    "        with open(os.path.join(SAVE_DIR,'response_traj.json'),'w') as f:\n",
    "            json.dump(flq_response_json,f)\n",
    "                \n",
    "else:\n",
    "    assert LOAD_TRAJ_RESPONSE == True\n",
    "    print(\"Loading prior fine location response\")\n",
    "    with open(os.path.join(SAVE_DIR,'response_traj.json'),'r') as f:\n",
    "        flq_response_json = json.load(f)\n",
    "    \n",
    "    flq_response_json = {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json.items()}\n",
    "    groupids =  {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json['groupids'].items()}\n",
    "    trajectories =  {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json['trajectories'].items()}\n",
    "\n",
    "\n",
    "world_trajectories = {}        \n",
    "for k,v in trajectories.items():\n",
    "    world_trajectories[k] = []\n",
    "    for l in v:\n",
    "        world_trajectories[k].append(utils.pix2world(scgraph.graph.nodes[l]['pos']))\n",
    "\n",
    "print(f'OUTPUT TRAJECTORIES:{trajectories}')\n",
    "print(f'GROUP:{flq_response_json[\"groupids\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "world_trajectories = {'robot': [[4.785913853317808, -6.094902723735679],\n",
    "  [1.7561117578579681, -6.282840466926341],\n",
    "  [1.6895227008148943, 0.9715564202333073],\n",
    "  [-1.340279394644945, -2.3925291828795885]],\n",
    " 'human1': [[1.7561117578579681, -6.282840466926341]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing to HuNavSim files\n",
    "print(\"ADDING TRAJECTORIES TO SIM YAML FILES\")\n",
    "agents_yaml = {'hunav_loader': {'ros__parameters': {'map': LOCATION,\n",
    "   'publish_people': True,\n",
    "   'agents': []}}}\n",
    "blank_human = {'id': None,\n",
    "    'skin': 0,\n",
    "    'behavior': 0,\n",
    "    'group_id': -1,\n",
    "    'max_vel': 1.5,\n",
    "    'radius': 0.4,\n",
    "    'init_pose': {'x': None, 'y': None, 'z': 1.25, 'h': 0.0},\n",
    "    'goal_radius': 0.3,\n",
    "    'cyclic_goals': False,\n",
    "    'goals': [],\n",
    "    }\n",
    "agents = {}\n",
    "\n",
    "\n",
    "for i in range(len(trajectories.keys())-1):\n",
    "    agents_yaml['hunav_loader']['ros__parameters']['agents'].append(f'agent{i}')\n",
    "    agents[f'agent{i}'] = copy.deepcopy(blank_human)\n",
    "    agents[f'agent{i}']['id'] = i\n",
    "    agents[f'agent{i}']['behavior'] = 7+i\n",
    "    agents[f'agent{i}']['group_id'] = groupids[f'human{i+1}']\n",
    "    for j,g in enumerate(world_trajectories[f'human{i+1}']):\n",
    "        if j == 0:\n",
    "            agents[f'agent{i}']['init_pose'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'z':1.25,\n",
    "                'h':0.0,\n",
    "            }\n",
    "            if len(world_trajectories[f'human{i+1}']) <= 1:\n",
    "                agents[f'agent{i}']['goals'].append(f'g{1}')\n",
    "                agents[f'agent{i}'][f'g{1}'] = {\n",
    "                    'x':g[0],\n",
    "                    'y':g[1],\n",
    "                    'h':1.25\n",
    "                }\n",
    "        else:\n",
    "            agents[f'agent{i}']['goals'].append(f'g{j}')\n",
    "            agents[f'agent{i}'][f'g{j}'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'h':1.25\n",
    "            }\n",
    "agents_yaml['hunav_loader']['ros__parameters'].update(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HUNAV_SIM_AGENTS_FILE,'w') as f:\n",
    "    yaml.dump(agents_yaml,f)\n",
    "\n",
    "with open(os.path.join(HUNAV_GAZEBO_WRAPPER_DIR,'config','robot.yaml'),'w') as f:\n",
    "    yaml.dump({\n",
    "    'x_pose': world_trajectories['robot'][0][0],\n",
    "    'y_pose': world_trajectories['robot'][0][1],\n",
    "    'roll': 0.0,\n",
    "    'pitch': 0.0,\n",
    "    'yaw': 3.14\n",
    "},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Querying LLM for Human Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_desc =  {k.lower().replace('_','').replace(' ',''): v for k,v in scq_response_json['humanbehavior'].items()}\n",
    "print(behav_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list(behav_desc.keys()))):\n",
    "    print(i)\n",
    "    btq = BTQuery()\n",
    "    if QUERY_BT:\n",
    "        print(f'Querying for Human {i+1}')\n",
    "        #print(behav_desc[f'human{i+1}']['Behavior Towards Robot'])\n",
    "        \n",
    "        btq_full_prompt = btq.get_full_prompt(behavior = behav_desc[f'human{i+1}'])#['Behavior Towards Robot'])\n",
    "        btq_response_json = model.get_response(messages = btq_full_prompt,format = 'json_object',expected_keys=btq.required_output_keys)\n",
    "        if SAVE_BT_RESPONSE:\n",
    "            print(f\"Saving BT {i+1} response\")\n",
    "            #with open(f'responses/reponse_bt_{i+1}.json','w') as f:\n",
    "            with open(os.path.join(SAVE_DIR,f'response_bt_{i+1}.json'),'w') as f:\n",
    "                json.dump(btq_response_json,f)\n",
    "    else:\n",
    "        assert LOAD_BT_RESPONSE == True\n",
    "        print(f'Loading prior BT {i+1} response')\n",
    "        with open(os.path.join(SAVE_DIR,f'response_bt_{i+1}.json'),'w') as f:\n",
    "            btq_response_json = json.load(f)\n",
    "    \n",
    "    print('================Proposed Behavior:================') \n",
    "    print(json.dumps(btq_response_json,indent=4))\n",
    "    print(\"Continuing...\")\n",
    "    bt_xml = btq_response_json['tree']\n",
    "    for k,v in btq_response_json.items():\n",
    "        if 'custom' in str.lower(k):\n",
    "            custom_node_requests.append(v)\n",
    "    print(os.path.join(HUNAV_SIM_BT_FOLDER,f'LLMBT_{i}.xml'))\n",
    "    with open(os.path.join(HUNAV_SIM_BT_FOLDER,f'LLMBT_{i}.xml'),'w') as f:\n",
    "        f.write(bt_xml)\n",
    "    print(f\"Wrote BT to LLMBT_{i}.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_desc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
