{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Scene graph and image of map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import re\n",
    "from PIL import Image as PILImage\n",
    "from Prompts import *\n",
    "import pickle as pkl\n",
    "import yaml\n",
    "from utils.config import *\n",
    "from utils.models import *\n",
    "import copy\n",
    "import subprocess\n",
    "import xml.etree.ElementTree as ET\n",
    "import ipywidgets a s widgets\n",
    "from IPython.display import display\n",
    "model = GPTModel(config = dict(\n",
    "    MODEL_NAME = MODEL\n",
    "))\n",
    "\n",
    "with open(os.path.join('locations',LOCATION,'scene_graph.json'),'r') as f:\n",
    "    scene_graph = json.load(f)\n",
    "\n",
    "scgraph = utils.SceneGraph(scene_graph)\n",
    "encoded_img = utils.encode_image(os.path.join('locations',LOCATION,'scene_graph.png'))\n",
    "node_types = []\n",
    "edge_types = []\n",
    "for node in scgraph.get_parent_nodes():\n",
    "    if scgraph.graph.nodes[node]['type'] not in node_types:\n",
    "        node_types.append(scgraph.graph.nodes[node]['type'])\n",
    "\n",
    "for edge in scgraph.graph.edges:\n",
    "    if scgraph.graph.edges[edge]['type'] not in edge_types:\n",
    "        edge_types.append(scgraph.graph.edges[edge]['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Querying LLM for Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scQ = ScenarioQuery()\n",
    "scQ_full_prompt = scQ.get_full_prompt(\n",
    "    context=CONTEXT,\n",
    "    task=TASK,\n",
    "    rough_scenario=ROUGH_SCENARIO,\n",
    "    location = LOCATION_DESC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: [{'type': 'text', 'text': 'Provide outputs strictly in JSON format'}]\n",
      "\u001b[0m\n",
      "\u001b[32muser: [{'type': 'text', 'text': '\\n                                      \\nSocial Navigation is a complex skill for a robot to accomplish and the appropriateness of the behavior of a robot is highly dependent on the task and the social context. \\nThus a robot’s social navigation capabilities must be thoroughly tested, and this is done by evaluating the robot’s behavior in a number of scenarios in a variety of contexts.\\nYou are a scenario designer. Your task is to generate scenarios to test the social navigation capabilities of a robot.\\nA Social Navigation [Scenario] is defined by:\\n    1. Scenario Description: very detailed description of the scenario. WHAT happens in the scenario and WHERE the scenario takes place. WHERE the robot and humans are located.\\n    2. Human Behavior:  how human interacts with the robot when it is visible, for e.g. Human 1 is scared of the robot and asks it to stop, Human 2 doesn\\'t notice the robot at all etc.\\nYour output description will be later used by an expert Behaviour tree designer to generate a Behavior Tree for each human in the scene. \\n\\nThe behavior tree designer is not allowed to modify the scenario and can only create behavior that can be generated using the following Actions and Conditions:\\n - Conditions\\n        - Check the visibility of the robot\\n        - Check if the human has reached their goal\\n        - Check if robot is saying any particular phrase\\n        - Check if the robot is currently moving\\n        - Check if the robot is blocking the human\\'s path\\n    \\n    - Actions:\\n        - Make the human perform a gesture.\\n        - Make the human perform normal navigation to reach its goal and treat the robot as a normal obstalce. This is regular behavior for humans.\\n        - Make the human look in the direction of the robot\\n        - Make the human follow the robot\\n        - Make the human scared of the robot and avoid it.\\n        - Make the human give way to the robot\\n        - Make the human move quickly towards the front of the robot and block the robot.\\n        \\n        NOTE: AT ANY GIVEN POINT OF TIME, THE HUMAN CAN ONlY PEFORM ANY ONE OF THE ABOVE ACTIONS.\\n\\nThe humans are only capable of performing the actions mentioned above.\\nUser will provide a [Social context], a [Task] that the robot needs to do, a description of the location and optionally a [Rough Scenario]. \\nYour generated scenario will be programmatically simulated through a pipeline into a scenario in the Gazebo physics simulator.\\nRules:\\n- The humans can say \"WAIT\", \"PROCEED\", \"EXCUSE ME\" to the robot  to aid in navigation. The robot can say \"WAIT\", \"PROCEED\", \"EXCUSE ME\", \"ACKNOWLEDGED\" to the humans to aid in navigation. \\n- When the user provides a Rough Scenario, ensure your final scenario is strictly aligned to the rough scenario\\n- The humans in the simulator are SIMPLIFIED OBJECTS that only can move in 2D, send and receive simple phrases, detect the robot, look at the robot, group together with other humans, navigate to a predefined goal and change their trajectory conditioned on the robot’s position and velocity.\\n- When using groups in the scenario, add all group members to the humans in the scenario. Having only 1 human with \\'INTERACTING WITH GROUP\\' task is incorrect.\\n\\nDesign a scenario relevant to the following specifications:\\n\\n[Social context]: Robot is a home assistant in a Singaporean old-age home and performs daily helpful duties for the residents\\n[Robot Task]: Deliver coffee\\n[Rough Scenario]: None\\n[Location]:   The home has a Kitchen, a Bedroom and a Living Room. A doorway connects the Kitchen to the Living Room and a Passageway connects the Living Room and the Doorway.\\n\\nYOU ADHERE TO THE FOLLOWING JSON FORMAT STRICTLY. \\n{\\n\\'Scenario Description\\': <very detailed description of the scenario >,\\n\\'Number of Humans\\': <Number of humans that are involved in the scenario>,\\n‘Human behavior\\': {\\n‘Human 1’: <Describe the behavior of Human 1>,\\n‘Human 2’: <Describe the behavior of Human 2>,\\n},\\n\\'Expected Robot Behavior\\': <Describe the behavior expected from the robot>\\n}\\n'}]\n",
      "\u001b[0m\n",
      "\u001b[34massistant: [{'type': 'text', 'text': '\\n            {\\'Scenario Description\\': \"The robot is trying to deliver coffee from the Kitchen to the Living Room and encounters one of the elderly residents entering the Kitchen from the Living Room through the  Doorway.\",\\n            \\'Number of Humans\\': 1,\\n            \\'Human Behavior\\':{\\n            \\'Human 1\\': Human 1 is going from going to kitchen from the living room. If the robot is very close-by, Human asks the robot to stop and waits (for a maximum of 5s) for the robot to stop, then continues navigating. Ignores the robot if it asks the human to wait.\\n            },\\n            \\'Expected Robot Behavior\\': \"The robot says \"I AM HERE\" to the resident. It waits for the resident to be well clear of the Doorway before going through the Doorway to the Living Room in a slow pace.\"\\n            }\\n            '}]\n",
      "\u001b[0m\n",
      "\u001b[32muser: [{'type': 'text', 'text': \"\\nDesign a scenario relevant to the following specifications:\\n\\n[Social Context]:Robot is an emergency response robot inside a warehouse in a disaster situation.\\n[Task]:The task of the robot is to guide humans to safety.\\n[Location]:The Location is a Small Warehouse consists of various racks and open packaging areas connected by Passageways. The Passageways also create Intersections.\\n[Rough Scenario]:None\\n\\nYOU ADHERE TO THE FOLLOWING JSON FORMAT STRICTLY. \\n{\\n'Scenario Description': <very detailed description of the scenario >,\\n'Number of Humans': <Number of humans that are involved in the scenario>,\\n‘Human behavior': {\\n‘Human 1’: <Describe the behavior of Human 1>,\\n‘Human 2’: <Describe the behavior of Human 2>,\\n},\\n'Expected Robot Behavior': <Describe the behavior expected from the robot>\\n}\\n\\n\"}]\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "utils.pretty_print_conversation(scQ_full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying LLM for scenario\n",
      "Querying LLM for scenario\n",
      "Querying LLM for scenario\n",
      "Querying LLM for scenario\n",
      "Querying LLM for scenario\n"
     ]
    }
   ],
   "source": [
    "if QUERY_SC: \n",
    "    proposals = []\n",
    "    for i in range(NUM_SCENARIO_PROPOSALS):\n",
    "        print(\"Querying LLM for scenario\")\n",
    "        scq_response_json = model.get_response(messages = scQ_full_prompt,format = \"json_object\",expected_keys=scQ.required_output_keys)\n",
    "        proposals.append(scq_response_json)\n",
    "if USE_HANDCRAFTED_SCENARIO:\n",
    "        print('USING HANDCRAFTED SCENARIO')\n",
    "        print('Scenario Description:')\n",
    "        scenario_desc = scenario_desc_hc\n",
    "        num_humans = num_humans_hc\n",
    "        behav_desc = behav_desc_hc\n",
    "\n",
    "if LOAD_SC:\n",
    "    print(\"Loading prior scenario response\")\n",
    "    with open(os.path.join(SAVE_DIR,'response_sc.json'),'r') as f:\n",
    "        scq_response_json = json.load(f)\n",
    "    \n",
    "    scq_response_json = {k.lower().replace('_','').replace(' ',''): v for k,v in scq_response_json.items()}\n",
    "    scenario_desc = scq_response_json['scenariodescription']\n",
    "    behav_desc =  {k.lower().replace('_','').replace(' ',''): v for k,v in scq_response_json['humanbehavior'].items()}\n",
    "    num_humans =  scq_response_json['numberofhumans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: The robot is navigating through the warehouse to guide two humans to a safe exit. The scenario starts with the robot in the central Passageway, moving towards the north exit where the humans are initially located near different racks. Human 1 is in the northeast corner of the warehouse, searching for a safe exit. Human 2 is closer to the west side, looking around anxiously.\n",
      "2: The robot is navigating through the passageways of the small warehouse, where it encounters two humans. Human 1 is located near an intersection where several racks meet, while Human 2 is in an open packaging area. The robot's task is to guide both humans toward a safety exit located at the end of one of the passageways.\n",
      "3: The robot is navigating through the warehouse, starting from the intersection near the packaging area and moving towards the safety exit located at the far end of the warehouse. It encounters two humans: Human 1 is near the packaging area, frantically looking around, while Human 2 is at a different intersection, confused and unsure of where to go. The robot aims to guide them both to safety through the passageways and intersections.\n",
      "4: The robot is in a small warehouse during a disaster situation. The warehouse consists of various racks and open packaging areas connected by passageways that create intersections. The robot is attempting to guide humans to safety through the designated safe exit. The robot starts at the intersection near the middle of the warehouse. Human 1 is located near the packaging area and is visibly panicking. Human 2 is near the racks at the northwest corner of the warehouse, unaware of the emergency situation.\n",
      "5: The robot is navigating through the Small Warehouse to guide two humans to safety after a disaster has occurred. The warehouse has various racks and open packaging areas connected by Passageways that create Intersections. Human 1 is located near the packaging area and is visibly anxious. Human 2 is near the racks and is injured, moving slowly. The robot starts in the central passageway and needs to reach both humans to provide guidance.\n"
     ]
    }
   ],
   "source": [
    "rb = widgets.RadioButtons(\n",
    "        options=list(range(1,NUM_SCENARIO_PROPOSALS+1)),\n",
    "        description='Choose a scenario from the following:',\n",
    "        layout={'width': 'max-content'}, \n",
    "        disabled=False\n",
    "    )\n",
    "for i,proposal in enumerate(proposals):\n",
    "    print(f\"\"\"{i+1}: {proposal['scenariodescription']}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688578558e074d8a8871b70075feffad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Choose a scenario from the following:', layout=Layout(width='max-content'), options=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(rb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving scenario response\n",
      "USING GENERATED SCENARIO\n",
      "The robot is positioned at the center of the warehouse near an intersection. The robot's task is to guide humans to a designated safety zone located at the north-eastern corner of the warehouse. Human 1 and Human 2 are initially present in separate open packaging areas on the western side. The robot proceeds to move towards each human, guide them to the safety zone, passing through the racks and intersections.\n",
      "Number of Humans:2\n",
      "Behaviors:{'Human 1': 'Human 1 is initially packing items and does not notice the robot at first. When the robot becomes visible, Human 1 follows the robot to the safety zone while glancing around occasionally.', 'Human 2': 'Human 2 is attempting to find an exit and moving around the packaging area in distress. When the robot becomes visible, Human 2 stops looking around and starts following the robot to the safety zone quickly.'}\n"
     ]
    }
   ],
   "source": [
    "if QUERY_SC:\n",
    "    scq_response_json = proposals[rb.value-1]\n",
    "    if SAVE_SC_RESPONSE:\n",
    "        print(\"Saving scenario response\")\n",
    "        with open(os.path.join(SAVE_DIR,f'response_sc.json'),'w') as f:\n",
    "            json.dump(scq_response_json,f)\n",
    "    print('USING GENERATED SCENARIO')\n",
    "    scenario_desc = scq_response_json['scenariodescription']\n",
    "    num_humans = scq_response_json['numberofhumans']\n",
    "    behav_desc = scq_response_json['humanbehavior']\n",
    "    \n",
    "    print(scenario_desc)\n",
    "    print(f'Number of Humans:{num_humans}')\n",
    "    print(f'Behaviors:{behav_desc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Querying LLM for Human and Robot Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flq = FLocationQuery()\n",
    "\n",
    "flq_full_prompt = flq.get_full_prompt(\n",
    "    scene_graph = str(scene_graph),\n",
    "    node_types = ','.join(node_types),\n",
    "    edge_types = ','.join(edge_types),\n",
    "    encoded_img = encoded_img,\n",
    "    sc_desc = scenario_desc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.pretty_print_conversation(flq_full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUERY_TRAJ:\n",
    "    print(\"Querying LLM for Trajectories\")\n",
    "    valid_trajectories = 0\n",
    "    all_trajectories_valid = False\n",
    "    payload = flq_full_prompt.copy()\n",
    "    while not all_trajectories_valid:\n",
    "        retries = -1\n",
    "        payload = flq_full_prompt.copy()\n",
    "        while retries <=3: #retry until trajectories are valid (max 3)\n",
    "            all_trajectories_valid = True\n",
    "            flq_response_json = model.get_response(messages = payload,format = 'json_object',expected_keys=flq.required_output_keys)\n",
    "            if not isinstance(flq_response_json,dict):\n",
    "                retries+=1\n",
    "                continue\n",
    "            trajectories = flq_response_json['trajectories']\n",
    "            groupids = flq_response_json['groupids']\n",
    "            traj_valid = True\n",
    "            #test connectivity\n",
    "            for k,v in trajectories.items():\n",
    "                print(f'Proposed Trajectory:{v}')\n",
    "                traj_valid,errors = scgraph.isvalidtrajectory(v)\n",
    "                if not traj_valid: #requery LLM with error message\n",
    "                    print(\"Disconnected Trajectory Output, Retrying\")\n",
    "                    valid_trajectories=0\n",
    "                    all_trajectories_valid = False\n",
    "                    reply = f\"\"\"\n",
    "                            You made the following mistakes:\n",
    "                            <ERRORS>\n",
    "                            Retry and Return the answer in the same JSON format.\n",
    "                            \"\"\"\n",
    "                    error_string = \"\"\n",
    "                    for err in errors:\n",
    "                        error_string+=f\"There is no edge connecting {err[0]} and {err[1]}!\"\n",
    "                        \n",
    "                    reply = reply.replace('<ERRORS>',error_string)\n",
    "                    payload.append(\n",
    "                        {\n",
    "                            \"role\":\"assistant\",\n",
    "                            \"content\":[\n",
    "                                {\n",
    "                                    \"type\":\"text\",\n",
    "                                    \"text\": str(flq_response_json)\n",
    "                                }\n",
    "                            ]\n",
    "                        }    \n",
    "                        )\n",
    "                    payload.append({\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\":\"text\",\n",
    "                                    \"text\": reply\n",
    "                                }\n",
    "                                    ]\n",
    "                                }\n",
    "                                )\n",
    "                    \n",
    "                    retries+=1\n",
    "                    break\n",
    "                \n",
    "                  \n",
    "\n",
    "                if traj_valid and traj_intersecting:\n",
    "                    valid_trajectories+=1\n",
    "        \n",
    "            if all_trajectories_valid:\n",
    "                break\n",
    "    groupids =  {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json['groupids'].items()}\n",
    "    trajectories =  {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json['trajectories'].items()}\n",
    "\n",
    "    if SAVE_TRAJ_RESPONSE:\n",
    "        print(\"Saving trajectory response\")\n",
    "        #with open('responses/reponse_traj.json','w') as f:\n",
    "        with open(os.path.join(SAVE_DIR,'response_traj.json'),'w') as f:\n",
    "            json.dump(flq_response_json,f)\n",
    "                \n",
    "else:\n",
    "    assert LOAD_TRAJ_RESPONSE == True\n",
    "    print(\"Loading prior fine location response\")\n",
    "    with open(os.path.join(SAVE_DIR,'response_traj.json'),'r') as f:\n",
    "        flq_response_json = json.load(f)\n",
    "    \n",
    "    flq_response_json = {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json.items()}\n",
    "    groupids =  {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json['groupids'].items()}\n",
    "    trajectories =  {k.lower().replace('_','').replace(' ',''): v for k,v in flq_response_json['trajectories'].items()}\n",
    "\n",
    "\n",
    "world_trajectories = {}        \n",
    "for k,v in trajectories.items():\n",
    "    world_trajectories[k] = []\n",
    "    for l in v:\n",
    "        world_trajectories[k].append(utils.pix2world(scgraph.graph.nodes[l]['pos']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'OUTPUT TRAJECTORIES:{trajectories}')\n",
    "print(f'GROUP:{flq_response_json[\"groupids\"]}')\n",
    "print(f'REASONING:{flq_response_json[\"reasoning\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "world_trajectories = {'robot': [[4.785913853317808, -6.094902723735679],\n",
    "  [1.7561117578579681, -6.282840466926341],\n",
    "  [1.6895227008148943, 0.9715564202333073],\n",
    "  [-1.340279394644945, -2.3925291828795885]],\n",
    " 'human1': [[1.7561117578579681, -6.282840466926341]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing to HuNavSim files\n",
    "print(\"ADDING TRAJECTORIES TO SIM YAML FILES\")\n",
    "agents_yaml = {'hunav_loader': {'ros__parameters': {'map': LOCATION,\n",
    "   'publish_people': True,\n",
    "   'agents': []}}}\n",
    "blank_human = {'id': None,\n",
    "    'skin': 0,\n",
    "    'behavior': 0,\n",
    "    'group_id': -1,\n",
    "    'max_vel': 1.5,\n",
    "    'radius': 0.4,\n",
    "    'init_pose': {'x': None, 'y': None, 'z': 1.25, 'h': 0.0},\n",
    "    'goal_radius': 0.3,\n",
    "    'cyclic_goals': False,\n",
    "    'goals': [],\n",
    "    }\n",
    "agents = {}\n",
    "\n",
    "\n",
    "for i in range(len(trajectories.keys())-1):\n",
    "    agents_yaml['hunav_loader']['ros__parameters']['agents'].append(f'agent{i}')\n",
    "    agents[f'agent{i}'] = copy.deepcopy(blank_human)\n",
    "    agents[f'agent{i}']['id'] = i\n",
    "    agents[f'agent{i}']['behavior'] = 7+i\n",
    "    agents[f'agent{i}']['group_id'] = groupids[f'human{i+1}']\n",
    "    for j,g in enumerate(world_trajectories[f'human{i+1}']):\n",
    "        if j == 0:\n",
    "            agents[f'agent{i}']['init_pose'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'z':1.25,\n",
    "                'h':0.0,\n",
    "            }\n",
    "            if len(world_trajectories[f'human{i+1}']) <= 1:\n",
    "                agents[f'agent{i}']['goals'].append(f'g{1}')\n",
    "                agents[f'agent{i}'][f'g{1}'] = {\n",
    "                    'x':g[0],\n",
    "                    'y':g[1],\n",
    "                    'h':1.25\n",
    "                }\n",
    "        else:\n",
    "            agents[f'agent{i}']['goals'].append(f'g{j}')\n",
    "            agents[f'agent{i}'][f'g{j}'] = {\n",
    "                'x':g[0],\n",
    "                'y':g[1],\n",
    "                'h':1.25\n",
    "            }\n",
    "agents_yaml['hunav_loader']['ros__parameters'].update(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HUNAV_SIM_AGENTS_FILE,'w') as f:\n",
    "    yaml.dump(agents_yaml,f)\n",
    "\n",
    "with open(os.path.join(HUNAV_GAZEBO_WRAPPER_DIR,'config','robot.yaml'),'w') as f:\n",
    "    yaml.dump({\n",
    "    'x_pose': world_trajectories['robot'][0][0],\n",
    "    'y_pose': world_trajectories['robot'][0][1],\n",
    "    'roll': 0.0,\n",
    "    'pitch': 0.0,\n",
    "    'yaw': 3.14\n",
    "},f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Querying LLM for Human Behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_desc =  {k.lower().replace('_','').replace(' ',''): v for k,v in scq_response_json['humanbehavior'].items()}\n",
    "print(behav_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list(behav_desc.keys()))):\n",
    "    print(i)\n",
    "    btq = BTQuery()\n",
    "    if QUERY_BT:\n",
    "        print(f'Querying for Human {i+1}')\n",
    "        #print(behav_desc[f'human{i+1}']['Behavior Towards Robot'])\n",
    "        \n",
    "        btq_full_prompt = btq.get_full_prompt(behavior = behav_desc[f'human{i+1}'])#['Behavior Towards Robot'])\n",
    "        btq_response_json = model.get_response(messages = btq_full_prompt,format = 'json_object',expected_keys=btq.required_output_keys,node_library = node_library)\n",
    "        if SAVE_BT_RESPONSE:\n",
    "            print(f\"Saving BT {i+1} response\")\n",
    "            #with open(f'responses/reponse_bt_{i+1}.json','w') as f:\n",
    "            with open(os.path.join(SAVE_DIR,f'response_bt_{i+1}.json'),'w') as f:\n",
    "                json.dump(btq_response_json,f)\n",
    "    else:\n",
    "        assert LOAD_BT_RESPONSE == True\n",
    "        print(f'Loading prior BT {i+1} response')\n",
    "        with open(os.path.join(SAVE_DIR,f'response_bt_{i+1}.json'),'w') as f:\n",
    "            btq_response_json = json.load(f)\n",
    "    \n",
    "    print('================Proposed Behavior:================') \n",
    "    print(json.dumps(btq_response_json,indent=4))\n",
    "    print(\"Continuing...\")\n",
    "    bt_xml = btq_response_json['tree']\n",
    "    for k,v in btq_response_json.items():\n",
    "        if 'custom' in str.lower(k):\n",
    "            custom_node_requests.append(v)\n",
    "    print(os.path.join(HUNAV_SIM_BT_FOLDER,f'LLMBT_{i}.xml'))\n",
    "    with open(os.path.join(HUNAV_SIM_BT_FOLDER,f'LLMBT_{i}.xml'),'w') as f:\n",
    "        f.write(bt_xml)\n",
    "    print(f\"Wrote BT to LLMBT_{i}.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = print(bt_xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_desc.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
